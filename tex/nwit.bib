@comment{AAA}

@inproceedings{Abad2018,
  doi = {10.1145/3210459.3210471},
  url = {https://doi.org/10.1145/3210459.3210471},
  year = {2018},
  month = jun,
  publisher = {{ACM}},
  author = {Zahra Shakeri Hossein Abad and Oliver Karras and Kurt Schneider and Ken Barker and Mike Bauer},
  title = {Task Interruption in Software Development Projects},
  booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
  abstract = {Multitasking has always been an inherent part of software development and is known as the primary source of interruptions due to task switching in software development teams. Developing software involves a mix of analytical and creative work, and requires a significant load on brain functions, such as working memory and decision making. Thus, task switching in the context of software development imposes a cognitive load that causes software developers to lose focus and concentration while working thereby taking a toll on productivity. To investigate the disruptiveness of task switching and interruptions in software development projects, and to understand the reasons for and perceptions of the disruptiveness of task switching we used a mixed-methods approach including a longitudinal data analysis on 4,910 recorded tasks of 17 professional software developers, and a survey of 132 software developers. We found that, compared to task-specific factors (e.g. priority, level, and temporal stage), contextual factors such as interruption type (e.g. self/external), time of day, and task type and context are a more potent determinant of task switching disruptiveness in software development tasks. Furthermore, while most survey respondents believe external interruptions are more disruptive than self-interruptions, the results of our retrospective analysis reveals otherwise. We found that self-interruptions (i.e. voluntary task switchings) are more disruptive than external interruptions and have a negative effect on the performance of the interrupted tasks. Finally, we use the results of both studies to provide a set of comparative vulnerability and interaction patterns which can be used as a mean to guide decision-making and forecasting the consequences of task switching in software development teams.}
}

@article{Akerblom2016,
  doi = {10.1145/2936313.2816717},
  url = {https://doi.org/10.1145/2936313.2816717},
  year = {2016},
  month = may,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {51},
  number = {2},
  pages = {114--128},
  author = {Beatrice {\AA}kerblom and Tobias Wrigstad},
  title = {Measuring polymorphism in Python programs},
  journal = {{ACM} {SIGPLAN} Notices},
  abstract = {Following the increased popularity of dynamic languages and their increased use in critical software, there have been many proposals to retrofit static type system to these languages to improve possibilities to catch bugs and improve performance. A key question for any type system is whether the types should be structural, for more expressiveness, or nominal, to carry more meaning for the programmer. For retrofitted type systems, it seems the current trend is using structural types. This paper attempts to answer the question to what extent this extra expressiveness is needed, and how the possible polymorphism in dynamic code is used in practise. We study polymorphism in 36 real-world open source Python programs and approximate to what extent nominal and structural types could be used to type these programs. The study is based on collecting traces from multiple runs of the programs and analysing the polymorphic degrees of targets at more than 7 million call-sites. Our results show that while polymorphism is used in all programs, the programs are to a great extent monomorphic. The polymorphism found is evenly distributed across libraries and program-specific code and occur both during program start-up and normal execution. Most programs contain a few ``megamorphic'' call-sites where receiver types vary widely. The non-monomorphic parts of the programs can to some extent be typed with nominal or structural types, but none of the approaches can type entire programs.}
}

@article{AlencarDaCosta2017,
  doi = {10.1007/s10664-017-9548-7},
  url = {https://doi.org/10.1007/s10664-017-9548-7},
  year = {2017},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {23},
  number = {2},
  pages = {835--904},
  author = {Daniel {Alencar da Costa} and Shane McIntosh and Christoph Treude and Uir{\'{a}} Kulesza and Ahmed E. Hassan},
  title = {The impact of rapid release cycles on the integration delay of fixed issues},
  journal = {Empirical Software Engineering},
  abstract = {The release frequency of software projects has increased in recent years. Adopters of so-called rapid releases---short release cycles, often on the order of weeks, days, or even hours---claim that they can deliver fixed issues (i.e., implemented bug fixes and new features) to users more quickly. However, there is little empirical evidence to support these claims. In fact, our prior work shows that code integration phases may introduce delays for rapidly releasing projects---98\% of the fixed issues in the rapidly releasing Firefox project had their integration delayed by at least one release. To better understand the impact that rapid release cycles have on the integration delay of fixed issues, we perform a comparative study of traditional and rapid release cycles. Our comparative study has two parts: (i) a quantitative empirical analysis of 72,114 issue reports from the Firefox project, and a (ii) qualitative study involving 37 participants, who are contributors of the Firefox, Eclipse, and ArgoUML projects. Our study is divided into quantitative and qualitative analyses. Quantitative analyses reveal that, surprisingly, fixed issues take a median of 54\% (57 days) longer to be integrated in rapid Firefox releases than the traditional ones. To investigate the factors that are related to integration delay in traditional and rapid release cycles, we train regression models that model whether a fixed issue will have its integration delayed or not. Our explanatory models achieve good discrimination (ROC areas of 0.80--0.84) and calibration scores (Brier scores of 0.05--0.16) for rapid and traditional releases. Our explanatory models indicate that (i) traditional releases prioritize the integration of backlog issues, while (ii) rapid releases prioritize issues that were fixed in the current release cycle. Complementary qualitative analyses reveal that participants' perception about integration delay is tightly related to activities that involve decision making, risk management, and team collaboration. Moreover, the allure of shipping fixed issues faster is a main motivator for adopting rapid release cycles among participants (although this motivation is not supported by our quantitative analysis). Furthermore, to explain why traditional releases deliver fixed issues more quickly, our participants point out the rush for integration in traditional releases and the increased time that is invested on polishing issues in rapid releases. Our results suggest that rapid release cycles may not be a silver bullet for the rapid delivery of new content to users. Instead, our results suggest that the benefits of rapid releases are increased software stability and user feedback.}
}

@inproceedings{Ali2020,
  doi = {10.1145/3379597.3387511},
  url = {https://doi.org/10.1145/3379597.3387511},
  year = {2020},
  month = jun,
  publisher = {{ACM}},
  author = {Rao Hamza Ali and Chelsea Parlett-Pelleriti and Erik Linstead},
  title = {Cheating Death: A Statistical Survival Analysis of Publicly Available Python Projects},
  booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
  abstract = {We apply survival analysis methods to a dataset of publicly-available software projects in order to examine the attributes that might lead to their inactivity over time. We ran a Kaplan-Meier analysis and fit a Cox Proportional-Hazards model to a subset of Software Heritage Graph Dataset, consisting of 3052 popular Python projects hosted on GitLab/GitHub, Debian, and PyPI, over a period of 165 months. We show that projects with repositories on multiple hosting services, a timeline of publishing major releases, and a good network of developers, remain healthy over time and should be worthy of the effort put in by developers and contributors.}
}

@inproceedings{Altadmri2015,
  doi = {10.1145/2676723.2677258},
  url = {https://doi.org/10.1145/2676723.2677258},
  year = {2015},
  month = feb,
  publisher = {{ACM}},
  author = {Amjad Altadmri and Neil C.C. Brown},
  title = {37 Million Compilations: Investigating Novice Programming Mistakes in Large-Scale Student Data},
  booktitle = {Proceedings of the 46th {ACM} Technical Symposium on Computer Science Education},
  abstract = {Previous investigations of student errors have typically focused on samples of hundreds of students at individual institutions. This work uses a year's worth of compilation events from over 250,000 students all over the world, taken from the large Blackbox data set. We analyze the frequency, time-to-fix, and spread of errors among users, showing how these factors inter-relate, in addition to their development over the course of the year. These results can inform the design of courses, textbooks and also tools to target the most frequent (or hardest to fix) errors.}
}

@inproceedings{Ameller2012,
  doi = {10.1109/re.2012.6345838},
  url = {https://doi.org/10.1109/re.2012.6345838},
  year = {2012},
  month = sep,
  publisher = {{IEEE}},
  author = {David Ameller and Claudia Ayala and Jordi Cabot and Xavier Franch},
  title = {How do software architects consider non-functional requirements: An exploratory study},
  booktitle = {2012 20th {IEEE} International Requirements Engineering Conference ({RE})},
  abstract = {Dealing with non-functional requirements (NFRs) has posed a challenge onto software engineers for many years. Over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. Knowing more about the state of the practice on these topics may benefit both practitioners' and researchers' daily work. A few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that NFRs have on daily architects' practices. This paper presents some of the findings of an empirical study based on 13 interviews with software architects. It addresses questions such as: who decides the NFRs, what types of NFRs matter to architects, how are NFRs documented, and how are NFRs validated. The results are contextualized with existing previous work.}
}

@article{Anda2009,
  doi = {10.1109/tse.2008.89},
  url = {https://doi.org/10.1109/tse.2008.89},
  year = {2009},
  month = may,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {35},
  number = {3},
  pages = {407--429},
  author = {B.C.D. Anda and D.I.K. Sj{\o}berg and Audris Mockus},
  title = {Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo reproducibilities. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, ldquolow,rdquo \"high,rdquo and ldquolowrdquo reproducibilities. Moreover, variability for predictable reasons is also included in the notion of reproducibility. We found that the observed outcome of the four development projects matched our expectations, which were formulated partially on the basis of SE folklore. Nevertheless, achieving more reproducibility in SE remains a great challenge for SE research, education, and industry.}
}

@inproceedings{Apel2011,
  doi = {10.1145/2025113.2025141},
  url = {https://doi.org/10.1145/2025113.2025141},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Sven Apel and J\"{o}rg Liebig and Benjamin Brandl and Christian Lengauer and Christian K\"{a}stner},
  title = {Semistructured Merge: Rethinking Merge in Revision Control Systems},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {An ongoing problem in revision control systems is how to resolve conflicts in a merge of independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems that inherit the strengths of both: the generality of unstructured systems and the expressiveness of structured systems. The idea is to provide structural information of the underlying software artifacts --- declaratively, in the form of annotated grammars. This way, a wide variety of languages can be supported and the information provided can assist in the automatic resolution of two classes of conflicts: ordering conflicts and semantic conflicts. The former can be resolved independently of the language and the latter using specific conflict handlers. We have been developing a tool that supports semistructured merge and conducted an empirical study on 24 software projects developed in Java, C\#, and Python comprising 180 merge scenarios. We found that semistructured merge reduces the number of conflicts in 60\% of the sample merge scenarios by, on average, 34\%, compared to unstructured merge. We found also that renaming is challenging in that it can increase the number of conflicts during semistructured merge, and that a combination of unstructured and semistructured merge is a pragmatic way to go.}
}

@comment{BBB}

@inproceedings{Balachandran2013,
  doi = {10.1109/icse.2013.6606642},
  url = {https://doi.org/10.1109/icse.2013.6606642},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Vipin Balachandran},
  title = {Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93\% of all the automatically generated comments. There is only 14.71\% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60\%-92\%, which is significantly better than a comparable method based on file change history.}
}

@article{Barnett2011,
  doi = {10.1145/1953122.1953145},
  url = {https://doi.org/10.1145/1953122.1953145},
  year = {2011},
  month = jun,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {54},
  number = {6},
  pages = {81--91},
  author = {Mike Barnett and Manuel F\"{a}hndrich and K. Rustan M. Leino and Peter M\"{u}ller and Wolfram Schulte and Herman Venter},
  title = {Specification and verification: the Spec\# experience},
  journal = {Communications of the {ACM}},
  abstract = {Can a programming language really help programmers write better programs?}
}

@inproceedings{Barr2012,
  doi = {10.1007/978-3-642-28872-2_22},
  url = {https://doi.org/10.1007/978-3-642-28872-2_22},
  year = {2012},
  publisher = {Springer Berlin Heidelberg},
  pages = {316--331},
  author = {Earl T. Barr and Christian Bird and Peter C. Rigby and Abram Hindle and Daniel M. German and Premkumar Devanbu},
  title = {Cohesive and Isolated Development with Branches},
  booktitle = {Proceedings of the 15th international conference on Fundamental Approaches to Software Engineering},
  abstract = {The adoption of distributed version control (DVC ), such as Git and Mercurial, in open-source software (OSS) projects has been explosive. Why is this and how are projects using DVC? This new generation of version control supports two important new features: distributed repositories and histories that preserve branches and merges. Through interviews with lead developers in OSS projects and a quantitative analysis of mined data from the histories of sixty project, we find that the vast majority of the projects now using DVC continue to use a centralized model of code sharing, while using branching much more extensively than before their transition to DVC. We then examine the Linux history in depth in an effort to understand and evaluate how branches are used and what benefits they provide. We find that they enable natural collaborative processes: DVC branching allows developers to collaborate on tasks in highly cohesive branches, while enjoying reduced interference from developers working on other tasks, even if those tasks are strongly coupled to theirs.}
}

@inproceedings{Barzilay2011,
  doi = {10.1145/2089131.2089135},
  url = {https://doi.org/10.1145/2089131.2089135},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Ohad Barzilay},
  title = {Example embedding},
  booktitle = {Proceedings of the 10th {SIGPLAN} symposium on New ideas, new paradigms, and reflections on programming and software - {ONWARD} {\textquotesingle}11},
  abstract = {Using code examples in professional software development is like teenage sex. Those who say they do it all the time are probably lying. Although it is natural, those who do it feel guilty. Finally, once they start doing it, they are often not too concerned with safety, they discover that it is going to take a while to get really good at it, and they realize they will have to come up with a bunch of new ways of doing it before they really figure it all out.}
}

@inproceedings{Beck2011,
  doi = {10.1145/2025113.2025162},
  url = {https://doi.org/10.1145/2025113.2025162},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Fabian Beck and Stephan Diehl},
  title = {On the congruence of modularity and code coupling},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {Software systems are modularized to make their inherent complexity manageable. While there exists a set of well-known principles that may guide software engineers to design the modules of a software system, we do not know which principles are followed in practice. In a study based on 16 open source projects, we look at different kinds of coupling concepts between source code entities, including structural dependencies, fan-out similarity, evolutionary coupling, code ownership, code clones, and semantic similarity. The congruence between these coupling concepts and the modularization of the system hints at the modularity principles used in practice. Furthermore, the results provide insights on how to support developers to modularize software systems.}
}

@inproceedings{Begel2014,
  doi = {10.1145/2568225.2568233},
  url = {https://doi.org/10.1145/2568225.2568233},
  year = {2014},
  month = may,
  publisher = {{ACM}},
  author = {Andrew Begel and Thomas Zimmermann},
  title = {Analyze this! 145 questions for data scientists in software engineering},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  abstract = {In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.}
}

@inproceedings{Beller2015,
  doi = {10.1145/2786805.2786843},
  url = {https://doi.org/10.1145/2786805.2786843},
  year = {2015},
  month = aug,
  publisher = {{ACM}},
  author = {Moritz Beller and Georgios Gousios and Annibale Panichella and Andy Zaidman},
  title = {When, how, and why developers (do not) test in their {IDEs}},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  abstract = {The research community in Software Engineering and Software Testing in particular builds many of its contributions on a set of mutually shared expectations. Despite the fact that they form the basis of many publications as well as open-source and commercial testing applications, these common expectations and beliefs are rarely ever questioned. For example, Frederic Brooks' statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the "Mythical Man Month" in 1975. With this paper, we report on the surprising results of a large-scale field study with 416 software engineers whose development activity we closely monitored over the course of five months, resulting in over 13 years of recorded work time in their integrated development environments (IDEs). Our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice: the majority of developers in our study does not test; developers rarely run their tests in the IDE; Test-Driven Development (TDD) is not widely practiced; and, last but not least, software developers only spend a quarter of their work time engineering tests, whereas they think they test half of their time.}
}

@article{BenAri2011,
  doi = {10.1016/j.jvlc.2011.04.004},
  url = {https://doi.org/10.1016/j.jvlc.2011.04.004},
  year = {2011},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {22},
  number = {5},
  pages = {375--384},
  author = {Mordechai Ben-Ari and Roman Bednarik and Ronit Ben-Bassat Levy and Gil Ebel and Andr{\'{e}}s Moreno and Niko Myller and Erkki Sutinen},
  title = {A decade of research and development on program animation: The Jeliot experience},
  journal = {Journal of Visual Languages {\&} Computing},
  abstract = {Jeliot is a program animation system for teaching and learning elementary programming that has been developed over the past decade, building on the Eliot animation system developed several years before. Extensive pedagogical research has been done on various aspects of the use of Jeliot including improvements in learning, effects on attention, and acceptance by teachers. This paper surveys this research and development, and summarizes the experience and the lessons learned.}
}

@inproceedings{Beniamini2017,
  doi = {10.1109/icpc.2017.18},
  url = {https://doi.org/10.1109/icpc.2017.18},
  year = {2017},
  month = may,
  publisher = {{IEEE}},
  author = {Gal Beniamini and Sarah Gingichashvili and Alon Klein Orbach and Dror G. Feitelson},
  title = {Meaningful Identifier Names: The Case of Single-Letter Variables},
  booktitle = {2017 {IEEE}/{ACM} 25th International Conference on Program Comprehension ({ICPC})},
  abstract = {It is widely accepted that variable names in computer programs should be meaningful, and that this aids program comprehension. "Meaningful" is commonly interpreted as favoring long descriptive names. However, there is at least some use of short and even single-letter names: using i in loops is very common, and we show (by extracting variable names from 1000 popular GitHub projects in 5 languages) that some other letters are also widely used. In addition, controlled experiments with different versions of the same functions (specifically, different variable names) failed to show significant differences in ability to modify the code. Finally, an online survey showed that certain letters are strongly associated with certain types and meanings. This implies that a single letter can in fact convey meaning. The conclusion from all this is that single letter variables can indeed be used beneficially in certain cases, leading to more concise code.}
}

@inproceedings{Bettenburg2008,
  doi = {10.1145/1453101.1453146},
  url = {https://doi.org/10.1145/1453101.1453146},
  year = {2008},
  publisher = {{ACM} Press},
  author = {Nicolas Bettenburg and Sascha Just and Adrian Schr\"{o}ter and Cathrin Weiss and Rahul Premraj and Thomas Zimmermann},
  title = {What makes a good bug report?},
  booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} International Symposium on Foundations of software engineering - {SIGSOFT} {\textquotesingle}08/{FSE}-16},
  abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.}
}

@inproceedings{Bird2011,
  doi = {10.1145/2025113.2025119},
  url = {https://doi.org/10.1145/2025113.2025119},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Christian Bird and Nachiappan Nagappan and Brendan Murphy and Harald Gall and Premkumar Devanbu},
  title = {Don{\textquotesingle}t touch my code!: examining the effects of ownership on software quality},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects: Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results.}
}

@article{Bluedorn1999,
  doi = {10.1037/0021-9010.84.2.277},
  url = {https://doi.org/10.1037/0021-9010.84.2.277},
  year = {1999},
  publisher = {American Psychological Association ({APA})},
  volume = {84},
  number = {2},
  pages = {277--285},
  author = {Allen C. Bluedorn and Daniel B. Turban and Mary Sue Love},
  title = {The effects of stand-up and sit-down meeting formats on meeting outcomes},
  journal = {Journal of Applied Psychology},
  abstract = {The effects of meeting format (standing or sitting) on meeting length and the quality of group decision making were investigated by comparing meeting outcomes for 56 five-member groups that conducted meetings in a standing format with 55 five-member groups that conducted meetings in a seated format. Sit-down meetings were 34\% longer than stand-up meetings, but they produced no better decisions than stand-up meetings. Significant differences were also obtained for satisfaction with the meeting and task information use during the meeting but not for synergy or commitment to the group's decision. The findings were generally congruent with meeting-management recommendations in the time-management literature, although the lack of a significant difference for decision quality was contrary to theoretical expectations. This contrary finding may have been due to differences between the temporal context in which this study was conducted and those in which other time constraint research has been conducted, thereby revealing a potentially important contingency-temporal context.}
}

@inproceedings{Brun2011,
  doi = {10.1145/2025113.2025139},
  url = {https://doi.org/10.1145/2025113.2025139},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Yuriy Brun and Reid Holmes and Michael D. Ernst and David Notkin},
  title = {Proactive detection of collaboration conflicts},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results. First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems. Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations. Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.}
}

@comment{CCC}

@inproceedings{Chen2016,
  doi = {10.1145/2901739.2901758},
  url = {https://doi.org/10.1145/2901739.2901758},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Tse-Hsun Chen and Weiyi Shang and Jinqiu Yang and Ahmed E. Hassan and Michael W. Godfrey and Mohamed Nasser and Parminder Flora},
  title = {An empirical study on the practice of maintaining object-relational mapping code in Java systems},
  booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
  abstract = {Databases have become one of the most important components in modern software systems. For example, web services, cloud computing systems, and online transaction processing systems all rely heavily on databases. To abstract the complexity of accessing a database, developers make use of Object-Relational Mapping (ORM) frameworks. ORM frameworks provide an abstraction layer between the application logic and the underlying database. Such abstraction layer automatically maps objects in Object-Oriented Languages to database records, which significantly reduces the amount of boilerplate code that needs to be written. Despite the advantages of using ORM frameworks, we observe several difficulties in maintaining ORM code (i.e., code that makes use of ORM frameworks) when cooperating with our industrial partner. After conducting studies on other open source systems, we find that such difficulties are common in other Java systems. Our study finds that i) ORM cannot completely encapsulate database accesses in objects or abstract the underlying database technology, thus may cause ORM code changes more scattered; ii) ORM code changes are more frequent than regular code, but there is a lack of tools that help developers verify ORM code at compilation time; iii) we find that changes to ORM code are more commonly due to performance or security reasons; however, traditional static code analyzers need to be extended to capture the peculiarities of ORM code in order to detect such problems. Our study highlights the hidden maintenance costs of using ORM frameworks, and provides some initial insights about potential approaches to help maintain ORM code. Future studies should carefully examine ORM code, especially given the rising use of ORM in modern software systems.}
}

@inproceedings{Cherubini2007,
  doi = {10.1145/1240624.1240714},
  url = {https://doi.org/10.1145/1240624.1240714},
  year = {2007},
  month = apr,
  publisher = {{ACM}},
  author = {Mauro Cherubini and Gina Venolia and Rob DeLine and Amy J. Ko},
  title = {Let{\textquotesingle}s go to the whiteboard: how and why software developers use drawings},
  booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
  abstract = {Software developers are rooted in the written form of their code, yet they often draw diagrams representing their code. Unfortunately, we still know little about how and why they create these diagrams, and so there is little research to inform the design of visual tools to support developers' work. This paper presents findings from semi-structured interviews that have been validated with a structured survey. Results show that most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings. Diagrams that documented design decisions were often externalized in these temporary drawings and then subsequently lost. Current visualization tools and the software development practices that we observed do not solve these issues, but these results suggest several directions for future research.}
}

@inproceedings{Chong2007,
  doi = {10.1109/icse.2007.87},
  url = {https://doi.org/10.1109/icse.2007.87},
  year = {2007},
  month = may,
  publisher = {{IEEE}},
  author = {Jan Chong and Tom Hurlbutt},
  title = {The Social Dynamics of Pair Programming},
  booktitle = {29th International Conference on Software Engineering ({ICSE}{\textquotesingle}07)},
  abstract = {This paper presents data from a four month ethnographic study of professional pair programmers from two software development teams. Contrary to the current conception of pair programmers, the pairs in this study did not hew to the separate roles of \"driver\" and \"navigator\". Instead, the observed programmers moved together through different phases of the task, considering and discussing issues at the same strategic \"range \" or level of abstraction and in largely the same role. This form of interaction was reinforced by frequent switches in keyboard control during pairing and the use of dual keyboards. The distribution of expertise among the members of a pair had a strong influence on the tenor of pair programming interaction. Keyboard control had a consistent secondary effect on decisionmaking within the pair. These findings have implications for software development managers and practitioners as well as for the design of software development tools.}
}

@inproceedings{Cinneide2012,
  doi = {10.1145/2372251.2372260},
  url = {https://doi.org/10.1145/2372251.2372260},
  year = {2012},
  publisher = {{ACM} Press},
  author = {Mel {\'{O}} Cinn{\'{e}}ide and Laurence Tratt and Mark Harman and Steve Counsell and Iman Hemati Moghadam},
  title = {Experimental assessment of software metrics using automated refactoring},
  booktitle = {Proceedings of the {ACM}-{IEEE} international symposium on Empirical software engineering and measurement - {ESEM} {\textquotesingle}12},
  abstract = {A large number of software metrics have been proposed in the literature, but there is little understanding of how these metrics relate to one another. We propose a novel experimental technique, based on search-based refactoring, to assess software metrics and to explore relationships between them. Our goal is not to improve the program being refactored, but to assess the software metrics that guide the automated refactoring through repeated refactoring experiments. We apply our approach to five popular cohesion metrics using eight real-world Java systems, involving 300,000 lines of code and over 3,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in 55\% of cases, and show how our approach can be used to reveal novel and surprising insights into the software metrics under investigation.}
}

@article{CruzLemus2009,
  doi = {10.1007/s10664-009-9106-z},
  url = {https://doi.org/10.1007/s10664-009-9106-z},
  year = {2009},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {14},
  number = {6},
  pages = {685--719},
  author = {Jos\'{e} A. Cruz-Lemus and Marcela Genero and M. Esperanza Manso and Sandro Morasca and Mario Piattini},
  title = {Assessing the understandability of {UML} statechart diagrams with composite states{\textemdash}A family of empirical studies},
  journal = {Empirical Software Engineering},
  abstract = {The main goal of this work is to present a family of empirical studies that we have carried out to investigate whether the use of composite states may improve the understandability of UML statechart diagrams derived from class diagrams. Our hypotheses derive from conventional wisdom, which says that hierarchical modeling mechanisms are helpful in mastering the complexity of a software system. In our research, we have carried out three empirical studies, consisting of five experiments in total. The studies differed somewhat as regards the size of the UML statechart models, though their size and the complexity of the models were chosen so that they could be analyzed by the subjects within a limited time period. The studies also differed with respect to the type of subjects (students vs. professionals), the familiarity of the subjects with the domains of the diagrams, and other factors. To integrate the results obtained from each of the five experiments, we performed a meta-analysis study which allowed us to take into account the differences between studies and to obtain the overall effect that the use of composite states has on the understandability of UML statechart diagrams throughout all the experiments. The results obtained are not completely conclusive. They cast doubts on the usefulness of composite states for a better understanding and memorizing of UML statechart diagrams. Composite states seem only to be helpful for acquiring knowledge from the diagrams. At any rate, it should be noted that these results are affected by the previous experience of the subjects on modeling, as well as by the size and complexity of the UML statechart diagrams we used, so care should be taken when generalizing our results.}
}

@comment{DDD}

@inproceedings{Dabbish2012,
  doi = {10.1145/2145204.2145396},
  url = {https://doi.org/10.1145/2145204.2145396},
  year = {2012},
  publisher = {{ACM} Press},
  author = {Laura Dabbish and Colleen Stuart and Jason Tsay and Jim Herbsleb},
  title = {Social coding in {GitHub}: transparency and collaboration in an open software repository},
  booktitle = {Proceedings of the {ACM} 2012 conference on Computer Supported Cooperative Work - {CSCW} {\textquotesingle}12},
  abstract = {Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation.}
}

@inproceedings{Dagenais2010,
  doi = {10.1145/1882291.1882312},
  url = {https://doi.org/10.1145/1882291.1882312},
  year = {2010},
  publisher = {{ACM} Press},
  author = {Barth{\'{e}}l{\'{e}}my Dagenais and Martin P. Robillard},
  title = {Creating and evolving developer documentation},
  booktitle = {Proceedings of the eighteenth {ACM} {SIGSOFT} international symposium on Foundations of software engineering - {FSE} {\textquotesingle}10},
  abstract = {Developer documentation helps developers learn frameworks and libraries. To better understand how documentation in open source projects is created and maintained, we performed a qualitative study in which we interviewed core contributors who wrote developer documentation and developers who read documentation. In addition, we studied the evolution of 19 documents by analyzing more than 1500 document revisions. We identified the decisions that contributors make, the factors influencing these decisions and the consequences for the project. Among many findings, we observed how working on the documentation could improve the code quality and how constant interaction with the projects' community positively impacted the documentation.}
}

@inproceedings{Dang2012,
  doi = {10.1109/icse.2012.6227111},
  url = {https://doi.org/10.1109/icse.2012.6227111},
  year = {2012},
  month = jun,
  publisher = {{IEEE}},
  author = {Yingnong Dang and Rongxin Wu and Hongyu Zhang and Dongmei Zhang and Peter Nobel},
  title = {{ReBucket}: A method for clustering duplicate crash reports based on call stack similarity},
  booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
  abstract = {Software often crashes. Once a crash happens, a crash report could be sent to software developers for investigation upon user permission. To facilitate efficient handling of crashes, crash reports received by Microsoft's Windows Error Reporting (WER) system are organized into a set of "buckets". Each bucket contains duplicate crash reports that are deemed as manifestations of the same bug. The bucket information is important for prioritizing efforts to resolve crashing bugs. To improve the accuracy of bucketing, we propose ReBucket, a method for clustering crash reports based on call stack matching. ReBucket measures the similarities of call stacks in crash reports and then assigns the reports to appropriate buckets based on the similarity values. We evaluate ReBucket using crash data collected from five widely-used Microsoft products. The results show that ReBucket achieves better overall performance than the existing methods. On average, the F-measure obtained by ReBucket is about 0.88.}
}

@inproceedings{Davis2019,
  doi = {10.1145/3338906.3338909},
  url = {https://doi.org/10.1145/3338906.3338909},
  year = {2019},
  month = aug,
  publisher = {{ACM}},
  author = {James C. Davis and Louis G. Michael IV and Christy A. Coghlan and Francisco Servant and Dongyoon Lee},
  title = {Why aren't regular expressions a lingua franca? {An} empirical study on the re-use and portability of regular expressions},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  abstract = {This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics? In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language. We experimentally evaluated the riskiness of this practice using a novel regex corpus---537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences. We report that developers’ belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15\% exhibit semantic differences across languages and 10\% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.}
}

@article{DeLucia2009,
  doi = {10.1007/s10664-009-9127-7},
  url = {https://doi.org/10.1007/s10664-009-9127-7},
  year = {2009},
  month = dec,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {15},
  number = {5},
  pages = {455--492},
  author = {Andrea De Lucia and Carmine Gravino and Rocco Oliveto and Genoveffa Tortora},
  title = {An experimental comparison of {ER} and {UML} class diagrams for data modelling},
  journal = {Empirical Software Engineering},
  abstract = {We present the results of three sets of controlled experiments aimed at analysing whether UML class diagrams are more comprehensible than ER diagrams during data models maintenance. In particular, we considered the support given by the two notations in the comprehension and interpretation of data models, comprehension of the change to perform to meet a change request, and detection of defects contained in a data model. The experiments involved university students with different levels of ability and experience. The results demonstrate that using UML class diagrams subjects achieved better comprehension levels. With regard to the support given by the two notations during maintenance activities the results demonstrate that the two notations give the same support, while in general UML class diagrams provide a better support with respect to ER diagrams during verification activities.}
}

@article{Dzidek2008,
  doi = {10.1109/tse.2008.15},
  url = {https://doi.org/10.1109/tse.2008.15},
  year = {2008},
  month = may,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {34},
  number = {3},
  pages = {407--432},
  author = {W.J. Dzidek and E. Arisholm and L.C. Briand},
  title = {A Realistic Empirical Evaluation of the Costs and Benefits of {UML} in Software Maintenance},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54\% increase in the functional correctness of changes (p=0.03), and an insignificant 7\% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56\%) - at the expense of an insignificant 14\% increase in development time caused by the overhead of updating the UML documentation (p=0.35).}
}

@comment{EEE}

@inproceedings{Eichberg2015,
  doi = {10.1145/2786805.2786865},
  url = {https://doi.org/10.1145/2786805.2786865},
  year = {2015},
  month = aug,
  publisher = {{ACM}},
  author = {Michael Eichberg and Ben Hermann and Mira Mezini and Leonid Glanz},
  title = {Hidden truths in dead software paths},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  abstract = {Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specific kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach based on the detection of infeasible paths in code that can discover a wide range of code smells ranging from useless code that hinders comprehension to real bugs. Code issues are identified by calculating the difference between the control-flow graph that contains all technically possible edges and the corresponding graph recorded while performing a more precise analysis using abstract interpretation. We have evaluated the approach using the Java Development Kit as well as the Qualitas Corpus (a curated collection of over 100 Java Applications) and were able to find thousands of issues across a wide range of categories.}
}

@article{ElEmam2001,
  doi = {10.1109/32.935855},
  url = {https://doi.org/10.1109/32.935855},
  year = {2001},
  month = jul,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {27},
  number = {7},
  pages = {630--650},
  author = {K. El Emam and S. Benlarbi and N. Goel and S.N. Rai},
  title = {The confounding effect of class size on the validity of object-oriented metrics},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.}
}

@comment{FFF}

@inproceedings{Ford2016,
  doi = {10.1145/2950290.2950331},
  url = {https://doi.org/10.1145/2950290.2950331},
  year = {2016},
  month = nov,
  publisher = {{ACM}},
  author = {Denae Ford and Justin Smith and Philip J. Guo and Chris Parnin},
  title = {Paradise unplugged: identifying barriers for female participation on stack overflow},
  booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
  abstract = {It is no secret that females engage less in programming fields than males. However, in online communities, such as Stack Overflow, this gender gap is even more extreme: only 5.8\% of contributors are female. In this paper, we use a mixed-methods approach to identify contribution barriers females face in online communities. Through 22 semi-structured interviews with a spectrum of female users ranging from non-contributors to a top 100 ranked user of all time, we identified 14 barriers preventing them from contributing to Stack Overflow. We then conducted a survey with 1470 female and male developers to confirm which barriers are gender related or general problems for everyone. Females ranked five barriers significantly higher than males. A few of these include doubts in the level of expertise needed to contribute, feeling overwhelmed when competing with a large number of users, and limited awareness of site features. Still, there were other barriers that equally impacted all Stack Overflow users or affected particular groups, such as industry programmers. Finally, we describe several implications that may encourage increased participation in the Stack Overflow community across genders and other demographics.}
}

@inproceedings{Ford2019,
  doi = {10.1109/icse-seis.2019.00014},
  url = {https://doi.org/10.1109/icse-seis.2019.00014},
  year = {2019},
  month = may,
  publisher = {{IEEE}},
  author = {Denae Ford and Mahnaz Behroozi and Alexander Serebrenik and Chris Parnin},
  title = {Beyond the Code Itself: How Programmers Really Look at Pull Requests},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering: Software Engineering in Society ({ICSE}-{SEIS})},
  abstract = {Developers in open source projects must make decisions on contributions from other community members, such as whether or not to accept a pull request. However, secondary factors-beyond the code itself-can influence those decisions. For example, signals from GitHub profiles, such as a number of followers, activity, names, or gender can also be considered when developers make decisions. In this paper, we examine how developers use these signals (or not) when making decisions about code contributions. To evaluate this question, we evaluate how signals related to perceived gender identity and code quality influenced decisions on accepting pull requests. Unlike previous work, we analyze this decision process with data collected from an eye-tracker. We analyzed differences in what signals developers said are important for themselves versus what signals they actually used to make decisions about others. We found that after the code snippet (x=57\%), the second place programmers spent their time fixating is on supplemental technical signals (x=32\%), such as previous contributions and popular repositories. Diverging from what participants reported themselves, we also found that programmers fixated on social signals more than recalled.}
}

@inproceedings{Fucci2016,
  doi = {10.1145/2961111.2962592},
  url = {https://doi.org/10.1145/2961111.2962592},
  year = {2016},
  month = sep,
  publisher = {{ACM}},
  author = {Davide Fucci and Giuseppe Scanniello and Simone Romano and Martin Shepperd and Boyce Sigweni and Fernando Uyaguari and Burak Turhan and Natalia Juristo and Markku Oivo},
  title = {An External Replication on the Effects of Test-driven Development Using a Multi-site Blind Analysis Approach},
  booktitle = {Proceedings of the 10th {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement},
  abstract = {Context: Test-driven development (TDD) is an agile practice claimed to improve the quality of a software product, as well as the productivity of its developers. A previous study (i.e., baseline experiment) at the University of Oulu (Finland) compared TDD to a test-last development (TLD) approach through a randomized controlled trial. The results failed to support the claims. Goal: We want to validate the original study results by replicating it at the University of Basilicata (Italy), using a different design. Method: We replicated the baseline experiment, using a crossover design, with 21 graduate students. We kept the settings and context as close as possible to the baseline experiment. In order to limit researchers bias, we involved two other sites (UPM, Spain, and Brunel, UK) to conduct blind analysis of the data. Results: The Kruskal-Wallis tests did not show any significant difference between TDD and TLD in terms of testing effort (p-value = .27), external code quality (p-value = .82), and developers' productivity (p-value = .83). Nevertheless, our data revealed a difference based on the order in which TDD and TLD were applied, though no carry over effect. Conclusions: We verify the baseline study results, yet our results raises concerns regarding the selection of experimental objects, particularly with respect to their interaction with the order in which of treatments are applied. We recommend future studies to survey the tasks used in experiments evaluating TDD. Finally, to lower the cost of replication studies and reduce researchers' bias, we encourage other research groups to adopt similar multi-site blind analysis approach described in this paper.}
}

@comment{GGG}

@inproceedings{Gauthier2013,
  doi = {10.1109/icse.2013.6606670},
  url = {https://doi.org/10.1109/icse.2013.6606670},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Francois Gauthier and Ettore Merlo},
  title = {Semantic smells and errors in access control models: A case study in {PHP}},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains.}
}

@article{Ghiotto2020,
  doi = {10.1109/tse.2018.2871083},
  url = {https://doi.org/10.1109/tse.2018.2871083},
  year = {2020},
  month = aug,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {46},
  number = {8},
  pages = {892--915},
  author = {Gleiph Ghiotto and Leonardo Murta and Marcio Barros and Andr\'{e} van der Hoek},
  title = {On the Nature of Merge Conflicts: A Study of 2,731 Open Source Java Projects Hosted by {GitHub}},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {When multiple developers change a software system in parallel, these concurrent changes need to be merged to all appear in the software being developed. Numerous merge techniques have been proposed to support this task, but none of them can fully automate the merge process. Indeed, it has been reported that as much as 10 to 20 percent of all merge attempts result in a merge conflict, meaning that a developer has to manually complete the merge. To date, we have little insight into the nature of these merge conflicts. What do they look like, in detail? How do developers resolve them? Do any patterns exist that might suggest new merge techniques that could reduce the manual effort? This paper contributes an in-depth study of the merge conflicts found in the histories of 2,731 open source Java projects. Seeded by the manual analysis of the histories of five projects, our automated analysis of all 2,731 projects: (1) characterizes the merge conflicts in terms of number of chunks, size, and programming language constructs involved, (2) classifies the manual resolution strategies that developers use to address these merge conflicts, and (3) analyzes the relationships between various characteristics of the merge conflicts and the chosen resolution strategies. Our results give rise to three primary recommendations for future merge techniques, that---when implemented---could on one hand help in automatically resolving certain types of conflicts and on the other hand provide the developer with tool-based assistance to more easily resolve other types of conflicts that cannot be automatically resolved.}
}

@inproceedings{Giger2011,
  doi = {10.1145/2024445.2024455},
  url = {https://doi.org/10.1145/2024445.2024455},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Emanuel Giger and Martin Pinzger and Harald Gall},
  title = {Using the Gini Coefficient for bug prediction in Eclipse},
  booktitle = {Proceedings of the 12th international workshop and the 7th annual {ERCIM} workshop on Principles on software evolution and software evolution - {IWPSE}-{EVOL} {\textquotesingle}11},
  abstract = {The Gini coefficient is a prominent measure to quantify the inequality of a distribution. It is often used in the field of economy to describe how goods, e.g., wealth or farmland, are distributed among people. We use the Gini coefficient to measure code ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i.e., carried out, by relatively few developers.}
}

@inproceedings{Gousios2016,
  doi = {10.1145/2884781.2884826},
  url = {https://doi.org/10.1145/2884781.2884826},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Georgios Gousios and Margaret-Anne Storey and Alberto Bacchelli},
  title = {Work practices and challenges in pull-based development},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering},
  abstract = {The pull-based development model is an emerging way of contributing to distributed software projects that is gaining enormous popularity within the open source software (OSS) world. Previous work has examined this model by focusing on projects and their owners---we complement it by examining the work practices of project contributors and the challenges they face. We conducted a survey with 645 top contributors to active OSS projects using the pull-based model on GitHub, the prevalent social coding site. We also analyzed traces extracted from corresponding GitHub repositories. Our research shows that: contributors have a strong interest in maintaining awareness of project status to get inspiration and avoid duplicating work, but they do not actively propagate information; communication within pull requests is reportedly limited to low-level concerns and contributors often use communication channels external to pull requests; challenges are mostly social in nature, with most reporting poor responsiveness from integrators; and the increased transparency of this setting is a confirmed motivation to contribute. Based on these findings, we present recommendations for practitioners to streamline the contribution process and discuss potential future research directions.}
}

@article{Graziotin2014,
  doi = {10.7717/peerj.289},
  url = {https://doi.org/10.7717/peerj.289},
  year = {2014},
  month = mar,
  publisher = {{PeerJ}},
  volume = {2},
  pages = {e289},
  author = {Daniel Graziotin and Xiaofeng Wang and Pekka Abrahamsson},
  title = {Happy software developers solve problems better: psychological measurements in empirical software engineering},
  journal = {{PeerJ}},
  abstract = {For more than thirty years, it has been claimed that a way to improve software developers' productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states---emotions and moods---deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.}
}

@article{Green1996,
  doi = {10.1006/jvlc.1996.0009},
  url = {https://doi.org/10.1006/jvlc.1996.0009},
  year = {1996},
  month = jun,
  publisher = {Elsevier {BV}},
  volume = {7},
  number = {2},
  pages = {131--174},
  author = {Thomas R. G. Green and Marian Petre},
  title = {Usability Analysis of Visual Programming Environments: A 'Cognitive Dimensions' Framework},
  journal = {Journal of Visual Languages {\&} Computing},
  abstract = {Abstract The cognitive dimensions framework is a broad-brush evaluation technique for interactive devices and for non-interactive notations. It sets out a small vocabulary of terms designed to capture the cognitively-relevant aspects of structure, and shows how they can be traded off against each other. The purpose of this paper is to propose the framework as an evaluation technique for visual programming environments. We apply it to two commercially-available dataflow languages (with further examples from other systems) and conclude that it is effective and insightful; other HCI-based evaluation techniques focus on different aspects and would make good complements. Insofar as the examples we used are representative, current VPLs are successful in achieving a good 'closeness of match', but designers need to consider the 'viscosity ' (resistance to local change) and the 'secondary notation' (possibility of conveying extra meaning by choice of layout, colour, etc.).}
}

@inproceedings{Gulzar2016,
  doi = {10.1145/2884781.2884813},
  url = {https://doi.org/10.1145/2884781.2884813},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Muhammad Ali Gulzar and Matteo Interlandi and Seunghyun Yoo and Sai Deep Tetali and Tyson Condie and Todd Millstein and Miryung Kim},
  title = {{BigDebug}: debugging primitives for interactive big data processing in Spark},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering},
  abstract = {Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's data-centers is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires re-thinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user.First, BigDebug's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BigDebug scales to terabytes and its record-level tracing incurs less than 25\% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100\% time saving compared to the baseline replay debugger. The results show that BigDebug supports debugging at interactive speeds with minimal performance impact.}
}

@comment{HHH}

@inproceedings{Hanenberg2010,
  doi = {10.1145/1869459.1869462},
  url = {https://doi.org/10.1145/1869459.1869462},
  year = {2010},
  publisher = {{ACM} Press},
  author = {Stefan Hanenberg},
  title = {An experiment about static and dynamic type systems},
  booktitle = {Proceedings of the {ACM} international conference on Object oriented programming systems languages and applications - {OOPSLA} {\textquotesingle}10},
  abstract = {Although static type systems are an essential part in teach-ing and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).}
}

@article{Hannay2010,
  doi = {10.1109/tse.2009.41},
  url = {https://doi.org/10.1109/tse.2009.41},
  year = {2010},
  month = jan,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {36},
  number = {1},
  pages = {61--80},
  author = {J.E. Hannay and E. Arisholm and H. Engvik and D.I.K. Sj{\o}berg},
  title = {Effects of Personality on Pair Programming},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting the job performance of software professionals both individually and in teams. However, research suggests that other human-related factors such as motivation, general mental ability, expertise, and task complexity also affect the performance in general. This paper reports on a study of the impact of the Big Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals in three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part. The results show that: (1) Our data do not confirm a meta-analysis-based model of the impact of certain personality traits on performance and (2) personality traits, in general, have modest predictive value on pair programming performance compared with expertise, task complexity, and country. We conclude that more effort should be spent on investigating other performance-related predictors such as expertise, and task complexity, as well as other promising predictors, such as programming skill and learning. We also conclude that effort should be spent on elaborating on the effects of personality on various measures of collaboration, which, in turn, may be used to predict and influence performance. Insights into such malleable, rather than static, factors may then be used to improve pair programming performance.}
}

@inproceedings{Harms2016,
  doi = {10.1145/2960310.2960314},
  url = {https://doi.org/10.1145/2960310.2960314},
  year = {2016},
  month = aug,
  publisher = {{ACM}},
  author = {Kyle James Harms and Jason Chen and Caitlin L. Kelleher},
  title = {Distractors in Parsons Problems Decrease Learning Efficiency for Young Novice Programmers},
  booktitle = {Proceedings of the 2016 {ACM} Conference on International Computing Education Research},
  abstract = {Parsons problems are an increasingly popular method for helping inexperienced programmers improve their programming skills. In Parsons problems, learners are given a set of programming statements that they must assemble into the correct order. Parsons problems commonly use distractors, extra statements that are not part of the solution. Yet, little is known about the effect distractors have on a learner's ability to acquire new programming skills. We present a study comparing the effectiveness of learning programming from Parsons problems with and without distractors. The results suggest that distractors decrease learning efficiency. We found that distractor participants showed no difference in transfer task performance compared to those without distractors. However, the distractors increased learners cognitive load, decreased their success at completing Parsons problems by 26\%, and increased learners' time on task by 14\%.}
}

@inproceedings{Hata2019,
  doi = {10.1109/icse.2019.00123},
  url = {https://doi.org/10.1109/icse.2019.00123},
  year = {2019},
  month = may,
  publisher = {{IEEE}},
  author = {Hideaki Hata and Christoph Treude and Raula Gaikovina Kula and Takashi Ishio},
  title = {9.6 Million Links in Source Code Comments: Purpose, Evolution, and Decay},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  abstract = {Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10\% of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.}
}

@inproceedings{Hemmati2013,
  doi = {10.1109/msr.2013.6624048},
  url = {https://doi.org/10.1109/msr.2013.6624048},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Hadi Hemmati and Sarah Nadi and Olga Baysal and Oleksii Kononenko and Wei Wang and Reid Holmes and Michael W. Godfrey},
  title = {The {MSR} Cookbook: Mining a decade of research},
  booktitle = {2013 10th Working Conference on Mining Software Repositories ({MSR})},
  abstract = {The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices.}
}

@inproceedings{Hermans2011,
  doi = {10.1145/1985793.1985855},
  url = {https://doi.org/10.1145/1985793.1985855},
  year = {2011},
  month = may,
  publisher = {{ACM}},
  author = {Felienne Hermans and Martin Pinzger and Arie van Deursen},
  title = {Supporting professional spreadsheet users by generating leveled dataflow diagrams},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  abstract = {Thanks to their flexibility and intuitive programming model, spreadsheets are widely used in industry, often for businesscritical applications. Similar to software developers, professional spreadsheet users demand support for maintaining and transferring their spreadsheets. In this paper, we first study the problems and information needs of professional spreadsheet users by means of a survey conducted at a large financial company. Based on these needs, we then present an approach that extracts this information from spreadsheets and presents it in a compact and easy to understand way, with leveled dataflow diagrams. Our approach comes with three different views on the dataflow that allow the user to analyze the dataflow diagrams in a top-down fashion. To evaluate the usefulness of the proposed approach, we conducted a series of interviews as well as nine case studies in an industrial setting. The results of the evaluation clearly indicate the demand for and usefulness of our approach in ease the understanding of spreadsheets.}
}

@inproceedings{Hermans2016,
  doi = {10.1109/icpc.2016.7503706},
  url = {https://doi.org/10.1109/icpc.2016.7503706},
  year = {2016},
  month = may,
  publisher = {{IEEE}},
  author = {Felienne Hermans and Efthimia Aivaloglou},
  title = {Do code smells hamper novice programming? A controlled experiment on Scratch programs},
  booktitle = {2016 {IEEE} 24th International Conference on Program Comprehension ({ICPC})},
  abstract = {Recently, block-based programming languages like Alice, Scratch and Blockly have become popular tools for programming education. There is substantial research showing that block-based languages are suitable for early programming education. But can block-based programs be smelly too? And does that matter to learners? In this paper we explore the code smells metaphor in the context of block-based programming language Scratch. We conduct a controlled experiment with 61 novice Scratch programmers, in which we divided the novices into three groups. One third receive a non-smelly program, while the other groups receive a program suffering from the Duplication or the Long Method smell respectively. All subjects then perform the same comprehension tasks on their program, after which we measure their time and correctness. The results of the experiment show that code smell indeed influence performance: subjects working on the program exhibiting code smells perform significantly worse, but the smells did not affect the time subjects needed. Investigating different types of tasks in more detail, we find that Long Method mainly decreases system understanding, while Duplication decreases the ease with which subjects modify Scratch programs.}
}

@inproceedings{Herzig2013,
  doi = {10.1109/icse.2013.6606585},
  url = {https://doi.org/10.1109/icse.2013.6606585},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Kim Herzig and Sascha Just and Andreas Zeller},
  title = {It{\textquotesingle}s not a bug, it{\textquotesingle}s a feature: How misclassification impacts bug prediction},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8\% of all bug reports to be misclassified - that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39\% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.}
}

@inproceedings{Hindle2012,
  doi = {10.1109/icsm.2012.6405278},
  url = {https://doi.org/10.1109/icsm.2012.6405278},
  year = {2012},
  month = sep,
  publisher = {{IEEE}},
  author = {Abram Hindle and Christian Bird and Thomas Zimmermann and Nachiappan Nagappan},
  title = {Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers?},
  booktitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
  abstract = {Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted matches the perception that Program Managers and Developers have about the effort put into addressing certain topics. We found that effort extracted from version control that was relevant to a topic often matched the perception of the managers and developers of what occurred at the time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements.}
}

@article{Hindle2016,
  doi = {10.1145/2902362},
  url = {https://doi.org/10.1145/2902362},
  year = {2016},
  month = apr,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {59},
  number = {5},
  pages = {122--131},
  author = {Abram Hindle and Earl T. Barr and Mark Gabel and Zhendong Su and Premkumar Devanbu},
  title = {On the naturalness of software},
  journal = {Communications of the {ACM}},
  abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.}
}

@inproceedings{Hofmeister2017,
  doi = {10.1109/saner.2017.7884623},
  url = {https://doi.org/10.1109/saner.2017.7884623},
  year = {2017},
  month = feb,
  publisher = {{IEEE}},
  author = {Johannes Hofmeister and Janet Siegmund and Daniel V. Holt},
  title = {Shorter identifier names take longer to comprehend},
  booktitle = {2017 {IEEE} 24th International Conference on Software Analysis,  Evolution and Reengineering ({SANER})},
  abstract = {Developers spend the majority of their time comprehending code, a process in which identifier names play a key role. Although many identifier naming styles exist, they often lack an empirical basis and it is not quite clear whether short or long identifier names facilitate comprehension. In this paper, we investigate the effect of different identifier naming styles (letters, abbreviations, words) on program comprehension, and whether these effects arise because of their length or their semantics. We conducted an experimental study with 72 professional C\# developers, who looked for defects in source-code snippets. We used a within-subjects design, such that each developer saw all three versions of identifier naming styles and we measured the time it took them to find a defect. We found that words lead to, on average, 19\% faster comprehension speed compared to letters and abbreviations, but we did not find a significant difference in speed between letters and abbreviations. The results of our study suggest that defects in code are more difficult to detect when code contains only letters and abbreviations. Words as identifier names facilitate program comprehension and can help to save costs and improve software quality.}
}

@inproceedings{Hundhausen2011,
  doi = {10.1145/1953163.1953201},
  url = {https://doi.org/10.1145/1953163.1953201},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Christopher D. Hundhausen and Pawan Agarwal and Michael Trevisan},
  title = {Online vs. face-to-face pedagogical code reviews},
  booktitle = {Proceedings of the 42nd {ACM} technical symposium on Computer science education - {SIGCSE} {\textquotesingle}11},
  abstract = {Given the increased importance of communication, teamwork, and critical thinking skills in the computing profession, we have been exploring studio-based instructional methods, in which students develop solutions and iteratively refine them through critical review by their peers and instructor. We have developed an adaptation of studio-based instruction for computing education called the pedagogical code review (PCR), which is modeled after the code inspection process used in the software industry. Unfortunately, PCRs are time-intensive, making them difficult to implement within a typical computing course. To address this issue, we have developed an online environment that allows PCRs to take place asynchronously outside of class. We conducted an empirical study that compared a CS 1 course with online PCRs against a CS 1 course with face-to-face PCRs. Our study had three key results: (a) in the course with face-to-face PCRs, student attitudes with respect to self-efficacy and peer learning were significantly higher; (b) in the course with face-to-face PCRs, students identified more substantive issues in their reviews; and (c) in the course with face-to-face PCRs, students were generally more positive about the value of PCRs. In light of our findings, we recommend specific ways online PCRs can be better designed.}
}

@comment{III}

@comment{JJJ}

@book{Jacobson2013,
  author = {Ivar Jacobson and Pan-Wei Ng and Paul E. McMahon and Ian Spence and Svante Lidman},
  title = {The Essence of Software Engineering: Applying the SEMAT Kernel},
  publisher = {Addison-Wesley Professional},
  year = {2013},
  isbn = {978-0321885951},
  abstract = {SEMAT (Software Engineering Methods and Theory) is an international initiative designed to identify a common ground, or universal standard, for software engineering. It is supported by some of the most distinguished contributors to the field. Creating a simple language to describe methods and practices, the SEMAT team expresses this common ground as a kernel---or framework---of elements essential to all software development. The Essence of Software Engineering introduces this kernel and shows how to apply it when developing software and improving a team's way of working. It is a book for software professionals, not methodologists. Its usefulness to development team members, who need to evaluate and choose the best practices for their work, goes well beyond the description or application of any single method.}
}

@article{Jorgensen2011,
  doi = {10.1109/tse.2010.78},
  url = {https://doi.org/10.1109/tse.2010.78},
  year = {2011},
  month = sep,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {37},
  number = {5},
  pages = {695--707},
  author = {Magne J{\o}rgensen and Stein Grimstad},
  title = {The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.}
}

@article{Jorgensen2012,
  doi = {10.1109/tse.2011.40},
  url = {https://doi.org/10.1109/tse.2011.40},
  year = {2012},
  month = may,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {38},
  number = {3},
  pages = {677--693},
  author = {Magne J{\o}rgensen and Stein Grimstad},
  title = {Software Development Estimation Biases: The Role of Interdependence},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i.e., with stronger emphasis on connectedness, social context, and relationships. We propose that this connection may be enabled by an activation of one's self-construal when engaging in effort estimation, and a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses.}
}

@comment{KKK}

@book{KanatAlexander2012,
  author = {Max Kanat-Alexander},
  title = {Code Simplicity: The Science of Software Development},
  publisher = {O'Reilly},
  year = {2012},
  isbn = {978-1449313890},
  abstract = {Good software development results in simple code. Unfortunately, much of the code existing in the world today is far too complex. This concise guide helps you understand the fundamentals of good software development through universal laws---principles you can apply to any programming language or project from here to eternity.}
}

@article{Kapser2008,
  doi = {10.1007/s10664-008-9076-6},
  url = {https://doi.org/10.1007/s10664-008-9076-6},
  year = {2008},
  month = jul,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {13},
  number = {6},
  pages = {645--692},
  author = {Cory J. Kapser and Michael W. Godfrey},
  title = {{\textquotedblleft}Cloning considered harmful{\textquotedblright} considered harmful: patterns of cloning in software},
  journal = {Empirical Software Engineering},
  abstract = {Literature on the topic of code cloning often asserts that duplicating code within a software system is a bad practice, that it causes harm to the system's design and should be avoided. However, in our studies, we have found significant evidence that cloning is often used in a variety of ways as a principled engineering tool. For example, one way to evaluate possible new features for a system is to clone the affected subsystems and introduce the new features there, in a kind of sandbox testbed. As features mature and become stable within the experimental subsystems, they can be migrated incrementally into the stable code base; in this way, the risk of introducing instabilities in the stable version is minimized. This paper describes several patterns of cloning that we have observed in our case studies and discusses the advantages and disadvantages associated with using them. We also examine through a case study the frequencies of these clones in two medium-sized open source software systems, the Apache web server and the Gnumeric spreadsheet application. In this study, we found that as many as 71\% of the clones could be considered to have a positive impact on the maintainability of the software system.}
}

@inproceedings{Kasi2013,
  doi = {10.1109/icse.2013.6606619},
  url = {https://doi.org/10.1109/icse.2013.6606619},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Bakhtiar Khan Kasi and Anita Sarma},
  title = {Cassandra: Proactive conflict minimization through optimized task scheduling},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. The main precept of workspace awareness tools has been to identify potential conflicts early, while changes are still small and easier to resolve. However, in this approach conflicts still occur and require developer time and effort to resolve. We present a novel conflict minimization technique that proactively identifies potential conflicts, encodes them as constraints, and solves the constraint space to recommend a set of conflict-minimal development paths for the team. Here we present a study of four open source projects to characterize the distribution of conflicts and their resolution efforts. We then explain our conflict minimization technique and the design and implementation of this technique in our prototype, Cassandra. We show that Cassandra would have successfully avoided a majority of conflicts in the four open source test subjects. We demonstrate the efficiency of our approach by applying the technique to a simulated set of scenarios with higher than normal incidence of conflicts.}
}

@inproceedings{Khomh2012,
  doi = {10.1109/msr.2012.6224279},
  url = {https://doi.org/10.1109/msr.2012.6224279},
  year = {2012},
  month = jun,
  publisher = {{IEEE}},
  author = {Foutse Khomh and Tejinder Dhaliwal and Ying Zou and Bram Adams},
  title = {Do faster releases improve software quality? An empirical case study of Mozilla Firefox},
  booktitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
  abstract = {Nowadays, many software companies are shifting from the traditional 18-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since shorter release cycles result in shorter testing periods. In this paper, we empirically study the development process of Mozilla Firefox in 2010 and 2011, a period during which the project transitioned to a shorter release cycle. We compare crash rates, median uptime, and the proportion of post-release bugs of the versions that had a shorter release cycle with those having a traditional release cycle, to assess the relation between release cycle length and the software quality observed by the end user. We found that (1) with shorter release cycles, users do not experience significantly more post-release bugs and (2) bugs are fixed faster, yet (3) users experience these bugs earlier during software execution (the program crashes earlier).}
}

@inproceedings{Kiefer2015,
  doi = {10.1145/2837476.2837481},
  url = {https://doi.org/10.1145/2837476.2837481},
  year = {2015},
  month = oct,
  publisher = {{ACM}},
  author = {Marc Kiefer and Daniel Warzel and Walter F. Tichy},
  title = {An empirical study on parallelism in modern open-source projects},
  booktitle = {Proceedings of the 2nd International Workshop on Software Engineering for Parallel Systems},
  abstract = {Writing parallel programs is hard, especially for inexperienced programmers. Parallel language features are still being added on a regular basis to most modern object-oriented languages and this trend is likely to continue. Being able to support developers with tools for writing and optimizing parallel programs requires a deep understanding of how programmers approach and implement parallelism. We present an empirical study of 135 parallel open-source projects in Java, C\# and C++ ranging from small (< 1000 lines of code) to very large (> 2M lines of code) codebases. We examine the projects to find out how language features, synchronization mechanisms, parallel data structures and libraries are used by developers to express parallelism. We also determine which common parallel patterns are used and how the implemented solutions compare to typical textbook advice. The results show that similar parallel constructs are used equally often across languages, but usage also heavily depends on how easy to use a certain language feature is. Patterns that do not map well to a language are much rarer compared to other languages. Bad practices are prevalent in hobby projects but also occur in larger projects.}
}

@inproceedings{Kim2013,
  doi = {10.1109/icse.2013.6606626},
  url = {https://doi.org/10.1109/icse.2013.6606626},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Dongsun Kim and Jaechang Nam and Jaewoo Song and Sunghun Kim},
  title = {Automatic patch generation learned from human-written patches},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs.}
}

@inproceedings{Kim2016,
  doi = {10.1145/2983990.2984031},
  url = {https://doi.org/10.1145/2983990.2984031},
  year = {2016},
  month = oct,
  publisher = {{ACM}},
  author = {Dohyeong Kim and Yonghwi Kwon and Peng Liu and I. Luk Kim and David Mitchel Perry and Xiangyu Zhang and Gustavo Rodriguez-Rivera},
  title = {Apex: automatic programming assignment error explanation},
  booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  abstract = {This paper presents Apex, a system that can automatically generate explanations for programming assignment bugs, regarding where the bugs are and how the root causes led to the runtime failures. It works by comparing the passing execution of a correct implementation (provided by the instructor) and the failing execution of the buggy implementation (submitted by the student). The technique overcomes a number of technical challenges caused by syntactic and semantic differences of the two implementations. It collects the symbolic traces of the executions and matches assignment statements in the two execution traces by reasoning about symbolic equivalence. It then matches predicates by aligning the control dependences of the matched assignment statements, avoiding direct matching of path conditions which are usually quite different. Our evaluation shows that Apex is every effective for 205 buggy real world student submissions of 4 programming assignments, and a set of 15 programming assignment type of buggy programs collected from stackoverflow.com, precisely pinpointing the root causes and capturing the causality for 94.5\% of them. The evaluation on a standard benchmark set with over 700 student bugs shows similar results. A user study in the classroom shows that Apex has substantially improved student productivity.}
}

@article{Kinshumann2011,
  doi = {10.1145/1965724.1965749},
  url = {https://doi.org/10.1145/1965724.1965749},
  year = {2011},
  month = jul,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {54},
  number = {7},
  pages = {111--116},
  author = {Kinshuman Kinshumann and Kirk Glerum and Steve Greenberg and Gabriel Aul and Vince Orgovan and Greg Nichols and David Grant and Gretchen Loihle and Galen Hunt},
  title = {Debugging in the (very) large: ten years of implementation and experience},
  journal = {Communications of the {ACM}},
  abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing of error reports coming from an installed base of a billion machines. WER has collected billions of error reports in 10 years of operation. It collects error data automatically and classifies errors into buckets, which are used to prioritize developer effort and report fixes to users. WER uses a progressive approach to data collection, which minimizes overhead for most reports yet allows developers to collect detailed information when needed. WER takes advantage of its scale to use error statistics as a tool in debugging; this allows developers to isolate bugs that cannot be found at smaller scale. WER has been designed for efficient operation at large scale: one pair of database servers records all the errors that occur on all Windows computers worldwide.}
}

@article{Kocaguneli2012,
  doi = {10.1109/tse.2011.111},
  url = {https://doi.org/10.1109/tse.2011.111},
  year = {2012},
  month = nov,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {38},
  number = {6},
  pages = {1403--1416},
  author = {Ekrem Kocaguneli and Tim Menzies and Jacky W. Keung},
  title = {On the Value of Ensemble Effort Estimation},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9${\times}$10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n = 13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods.}
}

@article{Krein2016,
  doi = {10.1109/tse.2015.2488625},
  url = {https://doi.org/10.1109/tse.2015.2488625},
  year = {2016},
  month = apr,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {42},
  number = {4},
  pages = {302--321},
  author = {Jonathan L. Krein and Lutz Prechelt and Natalia Juristo and Aziz Nanthaamornphong and Jeffrey C. Carver and Sira Vegas and Charles D. Knutson and Kevin D. Seppi and Dennis L. Eggett},
  title = {A Multi-Site Joint Replication of a Design Patterns Experiment Using Moderator Variables to Generalize across Contexts},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Context. Several empirical studies have explored the benefits of software design patterns, but their collective results are highly inconsistent. Resolving the inconsistencies requires investigating moderators---i.e., variables that cause an effect to differ across contexts. Objectives. Replicate a design patterns experiment at multiple sites and identify sufficient moderators to generalize the results across prior studies. Methods. We perform a close replication of an experiment investigating the impact (in terms of time and quality) of design patterns (Decorator and Abstract Factory) on software maintenance. The experiment was replicated once previously, with divergent results. We execute our replication at four universities---spanning two continents and three countries---using a new method for performing distributed replications based on closely coordinated, small-scale instances ("joint replication"). We perform two analyses: 1) a post-hoc analysis of moderators, based on frequentist and Bayesian statistics; 2) an a priori  analysis of the original hypotheses, based on frequentist statistics. Results. The main effect differs across the previous instances of the experiment and across the sites in our distributed replication. Our analysis of moderators (including developer experience and pattern knowledge) resolves the differences sufficiently to allow for cross-context (and cross-study) conclusions. The final conclusions represent 126 participants from five universities and 12 software companies, spanning two continents and at least four countries. Conclusions. The Decorator pattern is found to be preferable to a simpler solution during maintenance, as long as the developer has at least some prior knowledge of the pattern. For Abstract Factory, the simpler solution is found to be mostly equivalent to the pattern solution. Abstract Factory is shown to require a higher level of knowledge and/or experience than Decorator for the pattern to be beneficial.}
}

@comment{LLL}

@article{Levy2020,
  doi = {10.1093/cybsec/tyaa006},
  url = {https://doi.org/10.1093/cybsec/tyaa006},
  year = {2020},
  month = jan,
  publisher = {Oxford University Press ({OUP})},
  volume = {6},
  number = {1},
  author = {Karen Levy and Bruce Schneier},
  title = {Privacy threats in intimate relationships},
  journal = {Journal of Cybersecurity},
  abstract = {This article provides an overview of intimate threats: a class of privacy threats that can arise within our families, romantic partnerships, close friendships, and caregiving relationships. Many common assumptions about privacy are upended in the context of these relationships, and many otherwise effective protective measures fail when applied to intimate threats. Those closest to us know the answers to our secret questions, have access to our devices, and can exercise coercive power over us. We survey a range of intimate relationships and describe their common features. Based on these features, we explore implications for both technical privacy design and policy, and offer design recommendations for ameliorating intimate privacy risks.}
}

@inproceedings{Lewis2013,
  doi = {10.1109/icse.2013.6606583},
  url = {https://doi.org/10.1109/icse.2013.6606583},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Chris Lewis and Zhongpeng Lin and Caitlin Sadowski and Xiaoyan Zhu and Rong Ou and E. James Whitehead},
  title = {Does bug prediction support human developers? Findings from a Google case study},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.}
}

@inproceedings{Li2013,
  doi = {10.1109/icse.2013.6606646},
  url = {https://doi.org/10.1109/icse.2013.6606646},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Sihan Li and Hucheng Zhou and Haoxiang Lin and Tian Xiao and Haibo Lin and Wei Lin and Tao Xie},
  title = {A characteristic study on failures of production distributed data-parallel programs},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {SCOPE is adopted by thousands of developers from tens of different product teams in Microsoft Bing for daily web-scale data processing, including index building, search ranking, and advertisement display. A SCOPE job is composed of declarative SQL-like queries and imperative C\# user-defined functions (UDFs), which are executed in pipeline by thousands of machines. There are tens of thousands of SCOPE jobs executed on Microsoft clusters per day, while some of them fail after a long execution time and thus waste tremendous resources. Reducing SCOPE failures would save significant resources. This paper presents a comprehensive characteristic study on 200 SCOPE failures/fixes and 50 SCOPE failures with debugging statistics from Microsoft Bing, investigating not only major failure types, failure sources, and fixes, but also current debugging practice. Our major findings include (1) most of the failures (84.5\%) are caused by defects in data processing rather than defects in code logic; (2) table-level failures (22.5\%) are mainly caused by programmers' mistakes and frequent data-schema changes while row-level failures (62\%) are mainly caused by exceptional data; (3) 93\% fixes do not change data processing logic; (4) there are 8\% failures with root cause not at the failure-exposing stage, making current debugging practice insufficient in this case. Our study results provide valuable guidelines for future development of data-parallel programs. We believe that these guidelines are not limited to SCOPE, but can also be generalized to other similar data-parallel platforms.}
}

@inproceedings{Liao2016,
  doi = {10.1145/2960310.2960315},
  url = {https://doi.org/10.1145/2960310.2960315},
  year = {2016},
  month = aug,
  publisher = {{ACM}},
  author = {Soohyun Nam Liao and Daniel Zingaro and Michael A. Laurenzano and William G. Griswold and Leo Porter},
  title = {Lightweight, Early Identification of At-Risk {CS}1 Students},
  booktitle = {Proceedings of the 2016 {ACM} Conference on International Computing Education Research},
  abstract = {Being able to identify low-performing students early in the term may help instructors intervene or differently allocate course resources. Prior work in CS1 has demonstrated that clicker correctness in Peer Instruction courses correlates with exam outcomes and, separately, that machine learning models can be built based on early-term programming assessments. This work aims to combine the best elements of each of these approaches. We offer a methodology for creating models, based on in-class clicker questions, to predict cross-term student performance. In as early as week 3 in a 12-week CS1 course, this model is capable of correctly predicting students as being in danger of failing, or not, for 70\% of the students, with only 17\% of students misclassified as not at-risk when at-risk. Additional measures to ensure more broad applicability of the methodology, along with possible limitations, are explored.}
}

@inproceedings{Louis2020,
  doi = {10.1145/3377816.3381736},
  url = {https://doi.org/10.1145/3377816.3381736},
  year = {2020},
  month = jun,
  publisher = {{ACM}},
  author = {Annie Louis and Santanu Kumar Dash and Earl T. Barr and Michael D. Ernst and Charles Sutton},
  title = {Where should I comment my code?: a dataset and model for predicting locations that need comments},
  booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
  abstract = {Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74\% and recall of 13\% in identifying comment-worthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.}
}

@inproceedings{Lo2015,
  doi = {10.1145/2786805.2786809},
  url = {https://doi.org/10.1145/2786805.2786809},
  year = {2015},
  month = aug,
  publisher = {{ACM}},
  author = {David Lo and Nachiappan Nagappan and Thomas Zimmermann},
  title = {How practitioners perceive the relevance of software engineering research},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  abstract = {The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71\% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.}
}

@comment{MMM}

@article{Maalej2014,
  doi = {10.1145/2622669},
  url = {https://doi.org/10.1145/2622669},
  year = {2014},
  month = sep,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {23},
  number = {4},
  pages = {1--37},
  author = {Walid Maalej and Rebecca Tiarks and Tobias Roehm and Rainer Koschke},
  title = {On the Comprehension of Program Comprehension},
  journal = {{ACM} Transactions on Software Engineering and Methodology},
  abstract = {Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge. We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments. Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.}
}

@article{Malloy2018,
  doi = {10.1007/s10664-018-9637-2},
  url = {https://doi.org/10.1007/s10664-018-9637-2},
  year = {2018},
  month = jul,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {24},
  number = {2},
  pages = {751--778},
  author = {Brian A. Malloy and James F. Power},
  title = {An empirical analysis of the transition from Python 2 to Python 3},
  journal = {Empirical Software Engineering},
  abstract = {Python is one of the most popular and widely adopted programming languages in use today. In 2008 the Python developers introduced a new version of the language, Python 3.0, that was not backward compatible with Python 2, initiating a transitional phase for Python software developers. In this paper, we describe a study that investigates the degree to which Python software developers are making the transition from Python 2 to Python 3. We have developed a Python compliance analyser, PyComply, and have analysed a previously studied corpus of Python applications called Qualitas. We use PyComply to measure and quantify the degree to which Python 3 features are being used, as well as the rate and context of their adoption in the Qualitas corpus. Our results indicate that Python software developers are not exploiting the new features and advantages of Python 3, but rather are choosing to retain backward compatibility with Python 2. Moreover, Python developers are confining themselves to a language subset, governed by the diminishing intersection of Python 2, which is not under development, and Python 3, which is under development with new features being introduced as the language continues to evolve.}
}

@inproceedings{Marinescu2011,
  doi = {10.1145/2024445.2024456},
  url = {https://doi.org/10.1145/2024445.2024456},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Cristina Marinescu},
  title = {Are the classes that use exceptions defect prone?},
  booktitle = {Proceedings of the 12th international workshop and the 7th annual {ERCIM} workshop on Principles on software evolution and software evolution - {IWPSE}-{EVOL} {\textquotesingle}11},
  abstract = {Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently many empirical studies point out that sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if the design entities (classes) that use exceptions are more defect prone than the other classes. The results, based on analyzing three releases of Eclipse, show that indeed the classes that use exceptions are more defect prone than the other classes. Based on our results, developers are advertised to pay more attention to the way they handle exceptions.}
}

@article{Masood2020,
  doi = {10.1007/s10664-020-09876-x},
  url = {https://doi.org/10.1007/s10664-020-09876-x},
  year = {2020},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {25},
  number = {6},
  pages = {4962--5005},
  author = {Zainab Masood and Rashina Hoda and Kelly Blincoe},
  title = {How agile teams make self-assignment work: a grounded theory study},
  journal = {Empirical Software Engineering},
  abstract = {Self-assignment, a self-directed method of task allocation in which teams and individuals assign and choose work for themselves, is considered one of the hallmark practices of empowered, self-organizing agile teams. Despite all the benefits it promises, agile software teams do not practice it as regularly as other agile practices such as iteration planning and daily stand-ups, indicating that it is likely not an easy and straighforward practice. There has been very little empirical research on self-assignment. This Grounded Theory study explores how self-assignment works in agile projects. We collected data through interviews with 42 participants representing 28 agile teams from 23 software companies and supplemented these interviews with observations. Based on rigorous application of Grounded Theory analysis procedures such as open, axial, and selective coding, we present a comprehensive grounded theory of making self-assignment work that explains the (a) context and (b) causal conditions that give rise to the need for self-assignment, (c) a set of facilitating conditions that mediate how self-assignment may be enabled, (d) a set of constraining conditions that mediate how self-assignment may be constrained and which are overcome by a set of (e) strategies applied by agile teams, which in turn result in (f) a set of consequences, all in an attempt to make the central phenomenon, self-assignment, work. The findings of this study will help agile practitioners and companies understand different aspects of self-assignment and practice it with confidence regularly as a valuable practice. Additionally, it will help teams already practicing self-assignment to apply strategies to overcome the challenges they face on an everyday basis.}
}

@article{Mattmann2015,
  doi = {10.1007/s10723-015-9324-0},
  url = {https://doi.org/10.1007/s10723-015-9324-0},
  year = {2015},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {13},
  number = {1},
  pages = {19--34},
  author = {Chris A. Mattmann and Joshua Garcia and Ivo Krka and Daniel Popescu and Nenad Medvidovi\'{c}},
  title = {Revisiting the Anatomy and Physiology of the Grid},
  journal = {Journal of Grid Computing},
  abstract = {A domain-specific software architecture (DSSA) represents an effective, generalized, reusable solution to constructing software systems within a given application domain. In this paper, we revisit the widely cited DSSA for the domain of grid computing. We have studied systems in this domain over the last ten years. During this time, we have repeatedly observed that, while individual grid systems are widely used and deemed successful, the grid DSSA is actually underspecified to the point where providing a precise answer regarding what makes a software system a grid system is nearly impossible. Moreover, every one of the existing purported grid technologies actually violates the published grid DSSA. In response to this, based on an analysis of the source code, documentation, and usage of eighteen of the most pervasive grid technologies, we have significantly refined the original grid DSSA. We demonstrate that this DSSA much more closely matches the grid technologies studied. Our refinements allow us to more definitively identify a software system as a grid technology, and distinguish it from software libraries, middleware, and frameworks.}
}

@inproceedings{McGee2011,
  doi = {10.1109/re.2011.6051641},
  url = {https://doi.org/10.1109/re.2011.6051641},
  year = {2011},
  month = aug,
  publisher = {{IEEE}},
  author = {Sharon McGee and Des Greer},
  title = {Software requirements change taxonomy: Evaluation by case study},
  booktitle = {2011 {IEEE} 19th International Requirements Engineering Conference},
  abstract = {Although a number of requirements change classifications have been proposed in the literature, there is no empirical assessment of their practical value in terms of their capacity to inform change monitoring and management. This paper describes an investigation of the informative efficacy of a taxonomy of requirements change sources which distinguishes between changes arising from 'market', 'organisation', 'project vision', 'specification' and 'solution'. This investigation was effected through a case study where change data was recorded over a 16 month period covering the development lifecycle of a government sector software application. While insufficiency of data precluded an investigation of changes arising due to the change source of 'market', for the remainder of the change sources, results indicate a significant difference in cost, value to the customer and management considerations. Findings show that higher cost and value changes arose more often from 'organisation' and 'vision' sources; these changes also generally involved the co-operation of more stakeholder groups and were considered to be less controllable than changes arising from the 'specification' or 'solution' sources. Overall, the results suggest that monitoring and measuring change using this classification is a practical means to support change management, understanding and risk visibility.}
}

@inproceedings{McIntosh2011,
  doi = {10.1145/1985793.1985813},
  url = {https://doi.org/10.1145/1985793.1985813},
  year = {2011},
  month = may,
  publisher = {{ACM}},
  author = {Shane McIntosh and Bram Adams and Thanh H.D. Nguyen and Yasutaka Kamei and Ahmed E. Hassan},
  title = {An empirical study of build maintenance effort},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  abstract = {The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27\% overhead on source code development and a 44\% overhead on test development. Up to 79\% of source code developers and 89\% of test code developers are significantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22\% of source code developers and 24\% of test code developers.}
}

@article{McLeod2011,
  doi = {10.1145/1978802.1978803},
  url = {https://doi.org/10.1145/1978802.1978803},
  year = {2011},
  month = oct,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {43},
  number = {4},
  pages = {1--56},
  author = {Laurie McLeod and Stephen G. MacDonell},
  title = {Factors that affect software systems development project outcomes},
  journal = {{ACM} Computing Surveys},
  abstract = {Determining the factors that have an influence on software systems development and deployment project outcomes has been the focus of extensive and ongoing research for more than 30 years. We provide here a survey of the research literature that has addressed this topic in the period 1996--2006, with a particular focus on empirical analyses. On the basis of this survey we present a new classification framework that represents an abstracted and synthesized view of the types of factors that have been asserted as influencing project outcomes.}
}

@inproceedings{Meneely2011,
  doi = {10.1145/2025113.2025128},
  url = {https://doi.org/10.1145/2025113.2025128},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Andrew Meneely and Pete Rotella and Laurie Williams},
  title = {Does adding manpower also affect quality?: an empirical, longitudinal analysis},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics, we quantified characteristics of team expansion, including team size, expansion rate, expansion acceleration, and modularity with respect to department designations. We examined statistical correlations between our monthly team-level metrics and monthly product-level metrics. Our results indicate that increased team size and linear growth are correlated with later periods of better product quality. However, periods of accelerated team expansion are correlated with later periods of reduced software quality. Furthermore, our linear regression prediction model based on team metrics was able to predict the product's post-release failure rate within a 95\% prediction interval for 38 out of 40 months. Our analysis provides insight for project managers into how the expansion of development teams can impact product quality.}
}

@inproceedings{Meng2013,
  doi = {10.1109/icse.2013.6606596},
  url = {https://doi.org/10.1109/icse.2013.6606596},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Na Meng and Miryung Kim and Kathryn S. McKinley},
  title = {Lase: Locating and applying systematic edits by learning from examples},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99\% precision and 89\% recall, and transforms them with 91\% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.}
}

@inproceedings{Meyer2014,
  doi = {10.1145/2635868.2635892},
  url = {https://doi.org/10.1145/2635868.2635892},
  year = {2014},
  month = nov,
  publisher = {{ACM}},
  author = {Andr{\'{e}} N. Meyer and Thomas Fritz and Gail C. Murphy and Thomas Zimmermann},
  title = {Software developers{\textquotesingle} perceptions of productivity},
  booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
  abstract = {The better the software development community becomes at creating software, the more software the world seems to demand. Although there is a large body of research about measuring and investigating productivity from an organizational point of view, there is a paucity of research about how software developers, those at the front-line of software construction, think about, assess and try to improve their productivity. To investigate software developers' perceptions of software development productivity, we conducted two studies: a survey with 379 professional software developers to help elicit themes and an observational study with 11 professional software developers to investigate emergent themes in more detail. In both studies, we found that developers perceive their days as productive when they complete many or big tasks without significant interruptions or context switches. Yet, the observational data we collected shows our participants performed significant task and activity switching while still feeling productive. We analyze such apparent contradictions in our findings and use the analysis to propose ways to better support software developers in a retrospection and improvement of their productivity through the development of new tools and the sharing of best practices.}
}

@inproceedings{Miedema2021,
  doi = {10.1145/3446871.3469759},
  url = {https://doi.org/10.1145/3446871.3469759},
  year = {2021},
  month = aug,
  publisher = {{ACM}},
  author = {Daphne Miedema and Efthimia Aivaloglou and George Fletcher},
  title = {Identifying {SQL} Misconceptions of Novices: Findings from a Think-Aloud Study},
  booktitle = {Proceedings of the 17th {ACM} Conference on International Computing Education Research},
  abstract = {SQL is the most commonly taught database query language. While previous research has investigated the errors made by novices during SQL query formulation, the underlying causes for these errors have remained unexplored. Understanding the basic misconceptions held by novices which lead to these errors would help improve how we teach query languages to our students. In this paper we aim to identify the misconceptions that might be the causes of documented SQL errors that novices make. To this end, we conducted a qualitative think-aloud study to gather information on the thinking process of university students while solving query formulation problems. With the queries in hand, we analyzed the underlying causes for the errors made by our participants. In this paper we present the identified SQL misconceptions organized into four top-level categories: misconceptions based in previous course knowledge, generalization-based misconceptions, language-based misconceptions, and misconceptions due to an incomplete or incorrect mental model. A deep exploration of misconceptions can uncover gaps in instruction. By drawing attention to these, we aim to improve SQL education.}
}

@inproceedings{Miller2016,
  doi = {10.1145/2960310.2960327},
  url = {https://doi.org/10.1145/2960310.2960327},
  year = {2016},
  month = aug,
  publisher = {{ACM}},
  author = {Craig S. Miller and Amber Settle},
  title = {Some Trouble with Transparency: An Analysis of Student Errors with Object-oriented Python},
  booktitle = {Proceedings of the 2016 {ACM} Conference on International Computing Education Research},
  abstract = {We investigated implications of transparent mechanisms in the context of an introductory object-oriented programming course using Python. Here transparent mechanisms are those that reveal how the instance object in Python relates to its instance data. We asked students to write a new method for a provided Python class in an attempt to answer two research questions: 1) to what extent do Python's transparent OO mechanisms lead to student difficulties? and 2) what are common pitfalls in OO programming using Python that instructors should address? Our methodology also presented the correct answer to the students and solicited their comments on their submission. We conducted a content analysis to classify errors in the student submissions. We find that most students had difficulty with the instance (self) object, either by omitting the parameter in the method definition, by failing to use the instance object when referencing attributes of the object, or both. Reference errors in general were more common than other errors, including misplaced returns and indentation errors. These issues may be connected to problems with parameter passing and using dot-notation, which we argue are prerequisites for OO development in Python.}
}

@inproceedings{Mockus2010,
  doi = {10.1145/1882291.1882311},
  url = {https://doi.org/10.1145/1882291.1882311},
  year = {2010},
  publisher = {{ACM} Press},
  author = {Audris Mockus},
  title = {Organizational volatility and its effects on software defects},
  booktitle = {Proceedings of the eighteenth {ACM} {SIGSOFT} international symposium on Foundations of software engineering - {FSE}{\textquotesingle}10},
  abstract = {The key premise of an organization is to allow more efficient production, including production of high quality software. To achieve that, an organization defines roles and reporting relationships. Therefore, changes in organization's structure are likely to affect product's quality. We propose and investigate a relationship between developer-centric measures of organizational change and the probability of customer-reported defects in the context of a large software project. We find that the proximity to an organizational change is significantly associated with reductions in software quality. We also replicate results of several prior studies of software quality supporting findings that code, change, and developer characteristics affect fault-proneness. In contrast to prior studies we find that distributed development decreases quality. Furthermore, recent departures from an organization were associated with increased probability of customer-reported defects, thus demonstrating that in the observed context the organizational change reduces product quality.}
}

@article{Moe2010,
  doi = {10.1016/j.infsof.2009.11.004},
  url = {https://doi.org/10.1016/j.infsof.2009.11.004},
  year = {2010},
  month = may,
  publisher = {Elsevier {BV}},
  volume = {52},
  number = {5},
  pages = {480--491},
  author = {Nils Brede Moe and Torgeir Dings{\o}yr and Tore Dyb{\aa}},
  title = {A teamwork model for understanding an agile team: A case study of a Scrum project},
  journal = {Information and Software Technology},
  abstract = {Context: Software development depends significantly on team performance, as does any process that involves human interaction. Objective: Most current development methods argue that teams should self-manage. Our objective is thus to provide a better understanding of the nature of self-managing agile teams, and the teamwork challenges that arise when introducing such teams. Method: We conducted extensive fieldwork for 9months in a software development company that introduced Scrum. We focused on the human sensemaking, on how mechanisms of teamwork were understood by the people involved. Results: We describe a project through Dickinson and McIntyre's teamwork model, focusing on the interrelations between essential teamwork components. Problems with team orientation, team leadership and coordination in addition to highly specialized skills and corresponding division of work were important barriers for achieving team effectiveness. Conclusion: Transitioning from individual work to self-managing teams requires a reorientation not only by developers but also by management. This transition takes time and resources, but should not be neglected. In addition to Dickinson and McIntyre's teamwork components, we found trust and shared mental models to be of fundamental importance.}
}

@comment{NNN}

@article{Nagappan2008,
  doi = {10.1007/s10664-008-9062-z},
  url = {https://doi.org/10.1007/s10664-008-9062-z},
  year = {2008},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {13},
  number = {3},
  pages = {289--302},
  author = {Nachiappan Nagappan and E. Michael Maximilien and Thirumalesh Bhat and Laurie Williams},
  title = {Realizing quality improvement through test driven development: results and experiences of four industrial teams},
  journal = {Empirical Software Engineering},
  abstract = {Test-driven development (TDD) is a software development practice that has been used sporadically for decades. With this practice, a software engineer cycles minute-by-minute between writing failing unit tests and writing implementation code to pass those tests. Test-driven development has recently re-emerged as a critical enabling practice of agile software development methodologies. However, little empirical evidence supports or refutes the utility of this practice in an industrial context. Case studies were conducted with three development teams at Microsoft and one at IBM that have adopted TDD. The results of the case studies indicate that the pre-release defect density of the four products decreased between 40\% and 90\% relative to similar projects that did not use the TDD practice. Subjectively, the teams experienced a 15--35\% increase in initial development time after adopting TDD.}
}

@inproceedings{Nagappan2015,
  doi = {10.1145/2786805.2786834},
  url = {https://doi.org/10.1145/2786805.2786834},
  year = {2015},
  month = aug,
  publisher = {{ACM}},
  author = {Meiyappan Nagappan and Romain Robbes and Yasutaka Kamei and {\'{E}}ric Tanter and Shane McIntosh and Audris Mockus and Ahmed E. Hassan},
  title = {An empirical study of goto in C code from {GitHub} repositories},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  abstract = {It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is 'harmful' enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21${\pm}$5\%) and cleaning up resources at the end of a procedure (40.36${\pm}$5\%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice.}
}

@inproceedings{Nakshatri2016,
  doi = {10.1145/2901739.2903499},
  url = {https://doi.org/10.1145/2901739.2903499},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Suman Nakshatri and Maithri Hegde and Sahithi Thandra},
  title = {Analysis of exception handling patterns in Java projects},
  booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
  abstract = {Exception handling is a powerful tool provided by many pro- gramming languages to help developers deal with unforeseen conditions. Java is one of the few programming languages to enforce an additional compilation check on certain sub- classes of the Exception class through checked exceptions. As part of this study, empirical data was extracted from soft- ware projects developed in Java. The intent is to explore how developers respond to checked exceptions and identify common patterns used by them to deal with exceptions, checked or otherwise. Bloch's book - "Effective Java" [1] was used as reference for best practices in exception handling - these recommendations were compared against results from the empirical data. Results of this study indicate that most programmers ignore checked exceptions and leave them un- noticed. Additionally, it is observed that classes higher in the exception class hierarchy are more frequently used as compared to specific exception subclasses.}
}

@inproceedings{Near2016,
  doi = {10.1145/2884781.2884836},
  url = {https://doi.org/10.1145/2884781.2884836},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Joseph P. Near and Daniel Jackson},
  title = {Finding security bugs in web applications using a catalog of access control patterns},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering},
  abstract = {We propose a specification-free technique for finding missing security checks in web applications using a catalog of access control patterns in which each pattern models a common access control use case. Our implementation, SPACE, checks that every data exposure allowed by an application's code matches an allowed exposure from a security pattern in our catalog. The only user-provided input is a mapping from application types to the types of the catalog; the rest of the process is entirely automatic. In an evaluation on the 50 most watched Ruby on Rails applications on Github, SPACE reported 33 possible bugs---23 previously unknown security bugs, and 10 false positives.}
}

@inproceedings{Nussli2012,
  doi = {10.1145/2145204.2145371},
  url = {https://doi.org/10.1145/2145204.2145371},
  year = {2012},
  publisher = {{ACM} Press},
  author = {Marc-Antoine N\"{u}ssli and Patrick Jermann},
  title = {Effects of sharing text selections on gaze cross-recurrence and interaction quality in a pair programming task},
  booktitle = {Proceedings of the {ACM} 2012 conference on Computer Supported Cooperative Work - {CSCW} {\textquotesingle}12},
  abstract = {We present a dual eye-tracking study that demonstrates the effect of sharing selection among collaborators in a remote pair-programming scenario. Forty pairs of engineering students completed several program understanding tasks while their gaze was synchronously recorded. The coupling of the programmers' focus of attention was measured by a cross-recurrence analysis of gaze that captures how much programmers look at the same sequence of spots within a short time span. A high level of gaze cross-recurrence is typical for pairs who actively engage in grounding efforts to build and maintain shared understanding. As part of their grounding efforts, programmers may use text selection to perform collaborative references. Broadcast selections serve as indexing sites for the selector as they attract non-selector's gaze shortly after they become visible. Gaze cross-recurrence is highest when selectors accompany their selections with speech to produce a multimodal reference.}
}

@comment{OOO}

@article{Oliveira2020,
  doi = {10.1007/s10664-020-09820-z},
  url = {https://doi.org/10.1007/s10664-020-09820-z},
  year = {2020},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {25},
  number = {4},
  pages = {2519--2549},
  author = {Edson Oliveira and Eduardo Fernandes and Igor Steinmacher and Marco Cristo and Tayana Conte and Alessandro Garcia},
  title = {Code and commit metrics of developer productivity: a study on team leaders perceptions},
  journal = {Empirical Software Engineering},
  abstract = {Context Developer productivity is essential to the success of software development organizations. Team leaders use developer productivity information for managing tasks in a software project. Developer productivity metrics can be computed from software repositories data to support leaders' decisions. We can classify these metrics in code-based metrics, which rely on the amount of produced code, and commit-based metrics, which rely on commit activity. Although metrics can assist a leader, organizations usually neglect their usage and end up sticking to the leaders' subjective perceptions only. Objective We aim to understand whether productivity metrics can complement the leaders' perceptions. We also aim to capture leaders' impressions about relevance and adoption of productivity metrics in practice. Method This paper presents a multi-case empirical study performed in two organizations active for more than 18 years. Eight leaders of nine projects have ranked the developers of their teams by productivity. We quantitatively assessed the correlation of leaders' rankings versus metric-based rankings. As a complement, we interviewed leaders for qualitatively understanding the leaders' impressions about relevance and adoption of productivity metrics given the computed correlations. Results Our quantitative data suggest a greater correlation of the leaders' perceptions with code-based metrics when compared to commit-based metrics. Our qualitative data reveal that leaders have positive impressions of code-based metrics and potentially would adopt them. Conclusions Data triangulation of productivity metrics and leaders' perceptions can strengthen the organization conviction about productive developers and can reveal productive developers not yet perceived by team leaders and probably underestimated in the organization.}
}

@comment{PPP}

@inproceedings{Pankratius2012,
  doi = {10.1109/icse.2012.6227200},
  url = {https://doi.org/10.1109/icse.2012.6227200},
  year = {2012},
  month = jun,
  publisher = {{IEEE}},
  author = {Victor Pankratius and Felix Schmidt and Gilda Garreton},
  title = {Combining functional and imperative programming for multicore software: An empirical study evaluating Scala and Java},
  booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
  abstract = {Recent multi-paradigm programming languages combine functional and imperative programming styles to make software development easier. Given today's proliferation of multicore processors, parallel programmers are supposed to benefit from this combination, as many difficult problems can be expressed more easily in a functional style while others match an imperative style. Due to a lack of empirical evidence from controlled studies, however, important software engineering questions are largely unanswered. Our paper is the first to provide thorough empirical results by using Scala and Java as a vehicle in a controlled comparative study on multicore software development. Scala combines functional and imperative programming while Java focuses on imperative shared-memory programming. We study thirteen programmers who worked on three projects, including an industrial application, in both Scala and Java. In addition to the resulting 39 Scala programs and 39 Java programs, we obtain data from an industry software engineer who worked on the same project in Scala. We analyze key issues such as effort, code, language usage, performance, and programmer satisfaction. Contrary to popular belief, the functional style does not lead to bad performance. Average Scala run-times are comparable to Java, lowest run-times are sometimes better, but Java scales better on parallel hardware. We confirm with statistical significance Scala's claim that Scala code is more compact than Java code, but clearly refute other claims of Scala on lower programming effort and lower debugging effort. Our study also provides explanations for these observations and shows directions on how to improve multi-paradigm languages in the future.}
}

@inproceedings{Parnin2012,
  doi = {10.1109/icpc.2012.6240479},
  url = {https://doi.org/10.1109/icpc.2012.6240479},
  year = {2012},
  month = jun,
  publisher = {{IEEE}},
  author = {Chris Parnin and Spencer Rugaber},
  title = {Programmer information needs after memory failure},
  booktitle = {2012 20th {IEEE} International Conference on Program Comprehension ({ICPC})},
  abstract = {Despite its vast capacity and associative powers, the human brain does not deal well with interruptions. Particularly in situations where information density is high, such as during a programming task, recovering from an interruption requires extensive time and effort. Although modern program development environments have begun to recognize this problem, none of these tools take into account the brain's structure and limitations. In this paper, we present a conceptual framework for understanding the strengths and weaknesses of human memory, particularly with respect to it ability to deal with work interruptions. The framework explains empirical results obtained from experiments in which programmers were interrupted while working. Based on the framework, we discuss programmer information needs that development tools must satisfy and suggest several memory aids such tools could provide. We also describe our prototype implementation of these memory aids.}
}

@inproceedings{Patitsas2016,
  doi = {10.1145/2960310.2960312},
  url = {https://doi.org/10.1145/2960310.2960312},
  year = {2016},
  month = aug,
  publisher = {{ACM}},
  author = {Elizabeth Patitsas and Jesse Berlin and Michelle Craig and Steve Easterbrook},
  title = {Evidence That Computer Science Grades Are Not Bimodal},
  booktitle = {Proceedings of the 2016 {ACM} Conference on International Computing Education Research},
  abstract = {It is commonly thought that CS grades are bimodal. We statistically analyzed 778 distributions of final course grades from a large research university, and found only 5.8\% of the distributions passed tests of multimodality. We then devised a psychology experiment to understand why CS educators believe their grades to be bimodal. We showed 53 CS professors a series of histograms displaying ambiguous distributions and asked them to categorize the distributions. A random half of participants were primed to think about the fact that CS grades are commonly thought to be bimodal; these participants were more likely to label ambiguous distributions as \"bimodal\". Participants were also more likely to label distributions as bimodal if they believed that some students are innately predisposed to do better at CS. These results suggest that bimodal grades are instructional folklore in CS, caused by confirmation bias and instructor beliefs about their students.}
}

@inproceedings{Peng2021,
  doi = {10.1109/saner50967.2021.00012},
  url = {https://doi.org/10.1109/saner50967.2021.00012},
  year = {2021},
  month = mar,
  publisher = {{IEEE}},
  author = {Yun Peng and Yu Zhang and Mingzhe Hu},
  title = {An Empirical Study for Common Language Features Used in Python Projects},
  booktitle = {2021 {IEEE} International Conference on Software Analysis,  Evolution and Reengineering ({SANER})},
  abstract = {As a dynamic programming language, Python is widely used in many fields. For developers, various language features affect programming experience. For researchers, they affect the difficulty of developing tasks such as bug finding and compilation optimization. Former research has shown that programs with Python dynamic features are more change-prone. However, we know little about the use and impact of Python language features in real-world Python projects. To resolve these issues, we systematically analyze Python language features and propose a tool named PYSCAN to automatically identify the use of 22 kinds of common Python language features in 6 categories in Python source code. We conduct an empirical study on 35 popular Python projects from eight application domains, covering over 4.3 million lines of code, to investigate the the usage of these language features in the project. We find that single inheritance, decorator, keyword argument, for loops and nested classes are top 5 used language features. Meanwhile different domains of projects may prefer some certain language features. For example, projects in DevOps use exception handling frequently. We also conduct in-depth manual analysis to dig extensive using patterns of frequently but differently used language features: exceptions, decorators and nested classes/functions. We find that developers care most about ImportError when handling exceptions. With the empirical results and in-depth analysis, we conclude with some suggestions and a discussion of implications for three groups of persons in Python community: Python designers, Python compiler designers and Python developers.}
}

@inproceedings{PerezDeRosso2013,
  doi = {10.1145/2509578.2509584},
  url = {https://doi.org/10.1145/2509578.2509584},
  year = {2013},
  publisher = {{ACM} Press},
  author = {Santiago Perez De Rosso and Daniel Jackson},
  title = {What{\textquotesingle}s wrong with {Git}?},
  booktitle = {Proceedings of the 2013 {ACM} international symposium on New ideas,  new paradigms,  and reflections on programming {\&} software - Onward! {\textquotesingle}13},
  abstract = {It is commonly asserted that the success of a software development project, and the usability of the final product, depend on the quality of the concepts that underlie its design. Yet this hypothesis has not been systematically explored by researchers, and conceptual design has not played the central role in the research and teaching of software engineering that one might expect. As part of a new research project to explore conceptual design, we are engaging in a series of case studies. This paper reports on the early stages of our first study, on the Git version control system. Despite its widespread adoption, Git puzzles even experienced developers and is not regarded as easy to use. In an attempt to understand the root causes of its complexity, we analyze its conceptual model and identify some undesirable properties; we then propose a reworking of the conceptual model that forms the basis of (the first version of) Gitless, an ongoing effort to redesign Git and experiment with the effects of conceptual simplifications.}
}

@inproceedings{PerezDeRosso2016,
  doi = {10.1145/2983990.2984018},
  url = {https://doi.org/10.1145/2983990.2984018},
  year = {2016},
  month = oct,
  publisher = {{ACM}},
  author = {Santiago {Perez De Rosso} and Daniel Jackson},
  title = {Purposes, concepts, misfits, and a redesign of {Git}},
  booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming,  Systems,  Languages,  and Applications},
  abstract = {Git is a widely used version control system that is powerful but complicated. Its complexity may not be an inevitable consequence of its power but rather evidence of flaws in its design. To explore this hypothesis, we analyzed the design of Git using a theory that identifies concepts, purposes, and misfits. Some well-known difficulties with Git are described, and explained as misfits in which underlying concepts fail to meet their intended purpose. Based on this analysis, we designed a reworking of Git (called Gitless) that attempts to remedy these flaws. To correlate misfits with issues reported by users, we conducted a study of Stack Overflow questions. And to determine whether users experienced fewer complications using Gitless in place of Git, we conducted a small user study. Results suggest our approach can be profitable in identifying, analyzing, and fixing design problems.}
}

@inproceedings{Petre2013,
  doi = {10.1109/icse.2013.6606618},
  url = {https://doi.org/10.1109/icse.2013.6606618},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Marian Petre},
  title = {{UML} in practice},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {UML has been described by some as "the lingua franca of software engineering". Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry---if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.}
}

@inproceedings{Philip2012,
  doi = {10.1145/2145204.2145407},
  url = {https://doi.org/10.1145/2145204.2145407},
  year = {2012},
  publisher = {{ACM} Press},
  author = {Kavita Philip and Medha Umarji and Megha Agarwala and Susan Elliott Sim and Rosalva Gallardo-Valencia and Cristina V. Lopes and Sukanya Ratanotayanon},
  title = {Software reuse through methodical component reuse and amethodical snippet remixing},
  booktitle = {Proceedings of the {ACM} 2012 conference on Computer Supported Cooperative Work - {CSCW} {\textquotesingle}12},
  abstract = {Every method for developing software is a prescriptive model. Applying a deconstructionist analysis to methods reveals that there are two texts, or sets of assumptions and ideals: a set that is privileged by the method and a second set that is left out, or marginalized by the method. We apply this analytical lens to software reuse, a technique in software development that seeks to expedite one's own project by using programming artifacts created by others. By analyzing the methods prescribed by Component-Based Software Engineering (CBSE), we arrive at two texts: Methodical CBSE and Amethodical Remixing. Empirical data from four studies on code search on the web draws attention to four key points of tension: status of component boundaries; provenance of source code; planning and process; and evaluation criteria for candidate code. We conclude the paper with a discussion of the implications of this work for the limits of methods, structure of organizations that reuse software, and the design of search engines for source code.}
}

@inproceedings{Pietri2019,
  doi = {10.1109/msr.2019.00030},
  url = {https://doi.org/10.1109/msr.2019.00030},
  year = {2019},
  month = may,
  publisher = {{IEEE}},
  author = {Antoine Pietri and Diomidis Spinellis and Stefano Zacchiroli},
  title = {The Software Heritage Graph Dataset: Public Software Development Under One Roof},
  booktitle = {2019 {IEEE}/{ACM} 16th International Conference on Mining Software Repositories ({MSR})},
  abstract = {Software Heritage is the largest existing public archive of software source code and accompanying development history: it currently spans more than five billion unique source code files and one billion unique commits, coming from more than 80 million software projects. This paper introduces the Software Heritage graph dataset: a fully-deduplicated Merkle DAG representation of the Software Heritage archive. The dataset links together file content identifiers, source code directories, Version Control System (VCS) commits tracking evolution over time, up to the full states of VCS repositories as observed by Software Heritage during periodic crawls. The dataset's contents come from major development forges (including GitHub and GitLab), FOSS distributions (e.g., Debian), and language-specific package managers (e.g., PyPI). Crawling information is also included, providing timestamps about when and where all archived source code artifacts have been observed in the wild. The Software Heritage graph dataset is available in multiple formats, including downloadable CSV dumps and Apache Parquet files for local use, as well as a public instance on Amazon Athena interactive query service for ready-to-use powerful analytical processing. Source code file contents are cross-referenced at the graph leaves, and can be retrieved through individual requests using the Software Heritage archive API.},
}

@inproceedings{Porter2013,
  doi = {10.1145/2445196.2445250},
  url = {https://doi.org/10.1145/2445196.2445250},
  year = {2013},
  publisher = {{ACM} Press},
  author = {Leo Porter and Cynthia Bailey Lee and Beth Simon},
  title = {Halving fail rates using peer instruction},
  booktitle = {Proceeding of the 44th {ACM} technical symposium on Computer science education - {SIGCSE} {\textquotesingle}13},
  abstract = {Peer Instruction (PI) is a teaching method that supports student-centric classrooms, where students construct their own understanding through a structured approach featuring questions with peer discussions. PI has been shown to increase learning in STEM disciplines such as physics and biology. In this report we look at another indicator of student success the rate at which students pass the course or, conversely, the rate at which they fail. Evaluating 10 years of instruction of 4 different courses spanning 16 PI course instances, we find that adoption of the PI methodology in the classroom reduces fail rates by a per-course average of 61\% (20\% reduced to 7\%) compared to standard instruction (SI). Moreover, we also find statistically significant improvements within-instructor. For the same instructor teaching the same course, we find PI decreases the fail rate, on average, by 67\% (from 23\% to 8\%) compared to SI. As an in-situ study, we discuss the various threats to the validity of this work and consider implications of wide-spread adoption of PI in computing programs.}
}

@inproceedings{Posnett2011,
  doi = {10.1109/wcre.2011.33},
  url = {https://doi.org/10.1109/wcre.2011.33},
  year = {2011},
  month = oct,
  publisher = {{IEEE}},
  author = {Daryl Posnett and Abram Hindle and Premkumar Devanbu},
  title = {Got Issues? Do New Features and Code Improvements Affect Defects?},
  booktitle = {2011 18th Working Conference on Reverse Engineering},
  abstract = {There is a perception that when new features are added to a system that those added and modified parts of the source-code are more fault prone. Many have argued that new code and new features are defect prone due to immaturity, lack of testing, as well unstable requirements. Unfortunately most previous work does not investigate the link between a concrete requirement or new feature and the defects it causes, in particular the feature, the changed code and the subsequent defects are rarely investigated. In this paper we investigate the relationship between improvements, new features and defects recorded within an issue tracker. A manual case study is performed to validate the accuracy of these issue types. We combine defect issues and new feature issues with the code from version-control systems that introduces these features, we then explore the relationship of new features with the fault-proneness of their implementations. We describe properties and produce models of the relationship between new features and fault proneness, based on the analysis of issue trackers and version-control systems. We find, surprisingly, that neither improvements nor new features have any significant effect on later defect counts, when controlling for size and total number of changes.}
}

@inproceedings{Prabhu2011,
  doi = {10.1145/2063348.2063374},
  url = {https://doi.org/10.1145/2063348.2063374},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Prakash Prabhu and Yun Zhang and Soumyadeep Ghosh and David I. August and Jialu Huang and Stephen Beard and Hanjun Kim and Taewook Oh and Thomas B. Jablin and Nick P. Johnson and Matthew Zoufaly and Arun Raman and Feng Liu and David Walker},
  title = {A survey of the practice of computational science},
  booktitle = {State of the Practice Reports on - {SC} {\textquotesingle}11},
  abstract = {Computing plays an indispensable role in scientific research. Presently, researchers in science have different problems, needs, and beliefs about computation than professional programmers. In order to accelerate the progress of science, computer scientists must understand these problems, needs, and beliefs. To this end, this paper presents a survey of scientists from diverse disciplines, practicing computational science at a doctoral-granting university with very high re search activity. The survey covers many things, among them, prevalent programming practices within this scientific community, the importance of computational power in different fields, use of tools to enhance performance and soft ware productivity, computational resources leveraged, and prevalence of parallel computation. The results reveal several patterns that suggest interesting avenues to bridge the gap between scientific researchers and programming tools developers.}
}

@inproceedings{Pritchard2015,
  doi = {10.1145/2846680.2846681},
  url = {https://doi.org/10.1145/2846680.2846681},
  year = {2015},
  month = oct,
  publisher = {{ACM}},
  author = {David Pritchard},
  title = {Frequency distribution of error messages},
  booktitle = {Proceedings of the 6th Workshop on Evaluation and Usability of Programming Languages and Tools},
  abstract = {Which programming error messages are the most common? We investigate this question, motivated by writing error explanations for novices. We consider large data sets in Python and Java that include both syntax and run-time errors. In both data sets, after grouping essentially identical messages, the error message frequencies empirically resemble Zipf-Mandelbrot distributions. We use a maximum-likelihood approach to fit the distribution parameters. This gives one possible way to contrast languages or compilers quantitatively.}
}

@comment{QQQ}

@comment{RRR}

@inproceedings{Racheva2010,
  doi = {10.1109/re.2010.27},
  url = {https://doi.org/10.1109/re.2010.27},
  year = {2010},
  month = sep,
  publisher = {{IEEE}},
  author = {Zornitza Racheva and Maya Daneva and Klaas Sikkel and Andrea Herrmann and Roel Wieringa},
  title = {Do We Know Enough about Requirements Prioritization in Agile Projects: Insights from a Case Study},
  booktitle = {2010 18th {IEEE} International Requirements Engineering Conference},
  abstract = {Requirements prioritization is an essential mechanism of agile software development approaches. It maximizes the value delivered to the clients and accommodates changing requirements. This paper presents results of an exploratory cross-case study on agile prioritization and business value delivery processes in eight software organizations. We found that some explicit and fundamental assumptions of agile requirement prioritization approaches, as described in the agile literature on best practices, do not hold in all agile project contexts in our study. These are (i) the driving role of the client in the value creation process, (ii) the prevailing position of business value as a main prioritization criterion, (iii) the role of the prioritization process for project goal achievement. This implies that these assumptions have to be reframed and that the approaches to requirements prioritization for value creation need to be extended.}
}

@article{Ragkhitwetsagul2021,
  doi = {10.1109/tse.2019.2900307},
  url = {https://doi.org/10.1109/tse.2019.2900307},
  year = {2021},
  month = mar,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {47},
  number = {3},
  pages = {560--581},
  author = {Chaiyong Ragkhitwetsagul and Jens Krinke and Matheus Paixao and Giuseppe Bianco and Rocco Oliveto},
  title = {Toxic Code Snippets on Stack Overflow},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Online code clones are code fragments that are copied from software projects or online sources to Stack Overflow as examples. Due to an absence of a checking mechanism after the code has been copied to Stack Overflow, they can become toxic code snippets, e.g., they suffer from being outdated or violating the original software license. We present a study of online code clones on Stack Overflow and their toxicity by incorporating two developer surveys and a large-scale code clone detection. A survey of 201 high-reputation Stack Overflow answerers (33 percent response rate) showed that 131 participants (65 percent) have ever been notified of outdated code and 26 of them (20 percent) rarely or never fix the code. 138 answerers (69 percent) never check for licensing conflicts between their copied code snippets and Stack Overflow's CC BY-SA 3.0. A survey of 87 Stack Overflow visitors shows that they experienced several issues from Stack Overflow answers: mismatched solutions, outdated solutions, incorrect solutions, and buggy code. 85 percent of them are not aware of CC BY-SA 3.0 license enforced by Stack Overflow, and 66 percent never check for license conflicts when reusing code snippets. Our clone detection found online clone pairs between 72,365 Java code snippets on Stack Overflow and 111 open source projects in the curated Qualitas corpus. We analysed 2,289 non-trivial online clone candidates. Our investigation revealed strong evidence that 153 clones have been copied from a Qualitas project to Stack Overflow. We found 100 of them (66 percent) to be outdated, of which 10 were buggy and harmful for reuse. Furthermore, we found 214 code snippets that could potentially violate the license of their original software and appear 7,112 times in 2,427 GitHub projects.}
}

@inproceedings{Rahman2011,
  doi = {10.1145/1985793.1985860},
  url = {https://doi.org/10.1145/1985793.1985860},
  year = {2011},
  month = may,
  publisher = {{ACM}},
  author = {Foyzur Rahman and Premkumar Devanbu},
  title = {Ownership, experience and defects: a fine-grained study of authorship},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  abstract = {Recent research indicates that \"people\" factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a file might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a file might lead to more defects. Prior research considered this phenomenon at the level of modules or files, and thus does not tease apart and study the effect of contributions of different developers to each module or file. We exploit a modern version control system to examine this issue at a fine-grained level. Using version history, we examine contributions to code fragments that are actually repaired to fix bugs. Are these code fragments \"implicated\" in bugs the result of contributions from many? or from one? Does experience matter? What type of experience? We find that implicated code is more strongly associated with a single developer's contribution; our findings also indicate that an author's specialized experience in the target file is more important than general experience. Our findings suggest that quality control efforts could be profitably targeted at changes made by single developers with limited prior experience on that file.}
}

@inproceedings{Rahman2013,
  doi = {10.1109/icse.2013.6606589},
  url = {https://doi.org/10.1109/icse.2013.6606589},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Foyzur Rahman and Premkumar Devanbu},
  title = {How, and why, process metrics are better},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.}
}

@inproceedings{Rahman2020a,
  doi = {10.1145/3377811.3380409},
  url = {https://doi.org/10.1145/3377811.3380409},
  year = {2020},
  month = jun,
  publisher = {{ACM}},
  author = {Akond Rahman and Effat Farhana and Chris Parnin and Laurie Williams},
  title = {Gang of eight: a defect taxonomy for infrastructure as code scripts},
  booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering},
  abstract = {Defects in infrastructure as code (IaC) scripts can have serious consequences, for example, creating large-scale system outages. A taxonomy of IaC defects can be useful for understanding the nature of defects, and identifying activities needed to fix and prevent defects in IaC scripts. The goal of this paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by developing a defect taxonomy for IaC scripts through qualitative analysis. We develop a taxonomy of IaC defects by applying qualitative analysis on 1,448 defect-related commits collected from open source software (OSS) repositories of the Openstack organization. We conduct a survey with 66 practitioners to assess if they agree with the identified defect categories included in our taxonomy. We quantify the frequency of identified defect categories by analyzing 80,425 commits collected from 291 OSS repositories spanning across 2005 to 2019. Our defect taxonomy for IaC consists of eight categories, including a category specific to IaC called idempotency (i.e., defects that lead to incorrect system provisioning when the same IaC script is executed multiple times). We observe the surveyed 66 practitioners to agree most with idempotency. The most frequent defect category is configuration data i.e., providing erroneous configuration data in IaC scripts. Our taxonomy and the quantified frequency of the defect categories may help in advancing the science of IaC script quality.}
}

@inproceedings{Rigby2011,
  doi = {10.1145/1985793.1985867},
  url = {https://doi.org/10.1145/1985793.1985867},
  year = {2011},
  month = may,
  publisher = {{ACM}},
  author = {Peter C. Rigby and Margaret-Anne Storey},
  title = {Understanding broadcast based peer review on open source software projects},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  abstract = {Software peer review has proven to be a successful technique in open source software (OSS) development. In contrast to industry, where reviews are typically assigned to specific individuals, changes are broadcast to hundreds of potentially interested stakeholders. Despite concerns that reviews may be ignored, or that discussions will deadlock because too many uninformed stakeholders are involved, we find that this approach works well in practice. In this paper, we describe an empirical study to investigate the mechanisms and behaviours that developers use to find code changes they are competent to review. We also explore how stakeholders interact with one another during the review process. We manually examine hundreds of reviews across five high profile OSS projects. Our findings provide insights into the simple, community-wide techniques that developers use to effectively manage large quantities of reviews. The themes that emerge from our study are enriched and validated by interviewing long-serving core developers.}
}

@inproceedings{Rivers2016,
  doi = {10.1145/2960310.2960333},
  url = {https://doi.org/10.1145/2960310.2960333},
  year = {2016},
  month = aug,
  publisher = {{ACM}},
  author = {Kelly Rivers and Erik Harpstead and Ken Koedinger},
  title = {Learning Curve Analysis for Programming},
  booktitle = {Proceedings of the 2016 {ACM} Conference on International Computing Education Research},
  abstract = {The recent surge in interest in using educational data mining on student written programs has led to discoveries about which compiler errors students encounter while they are learning how to program. However, less attention has been paid to the actual code that students produce. In this paper, we investigate programming data by using learning curve analysis to determine which programming elements students struggle with the most when learning in Python. Our analysis extends the traditional use of learning curve analysis to include less structured data, and also reveals new possibilities for when to teach students new programming concepts. One particular discovery is that while we find evidence of student learning in some cases (for example, in function definitions and comparisons), there are other programming elements which do not demonstrate typical learning. In those cases, we discuss how further changes to the model could affect both demonstrated learning and our understanding of the different concepts that students learn.}
}

@article{Robillard2010,
  doi = {10.1007/s10664-010-9150-8},
  url = {https://doi.org/10.1007/s10664-010-9150-8},
  year = {2010},
  month = dec,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {16},
  number = {6},
  pages = {703--732},
  author = {Martin P. Robillard and Rob DeLine},
  title = {A field study of {API} learning obstacles},
  journal = {Empirical Software Engineering},
  abstract = {Large APIs can be hard to learn, and this can lead to decreased programmer productivity. But what makes APIs hard to learn? We conducted a mixed approach, multi-phased study of the obstacles faced by Microsoft developers learning a wide variety of new APIs. The study involved a combination of surveys and in-person interviews, and collected the opinions and experiences of over 440 professional developers. We found that some of the most severe obstacles faced by developers learning new APIs pertained to the documentation and other learning resources. We report on the obstacles developers face when learning new APIs, with a special focus on obstacles related to API documentation. Our qualitative analysis elicited five important factors to consider when designing API documentation: documentation of intent; code examples; matching APIs with scenarios; penetrability of the API; and format and presentation. We analyzed how these factors can be interpreted to prioritize API documentation development efforts}
}

@article{Rossbach2010,
  doi = {10.1145/1837853.1693462},
  url = {https://doi.org/10.1145/1837853.1693462},
  year = {2010},
  month = may,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {45},
  number = {5},
  pages = {47--56},
  author = {Christopher J. Rossbach and Owen S. Hofmann and Emmett Witchel},
  title = {Is transactional programming actually easier?},
  journal = {{ACM} {SIGPLAN} Notices},
  abstract = {Chip multi-processors (CMPs) have become ubiquitous, while tools that ease concurrent programming have not. The promise of increased performance for all applications through ever more parallel hardware requires good tools for concurrent programming, especially for average programmers. Transactional memory (TM) has enjoyed recent interest as a tool that can help programmers program concurrently. The transactional memory (TM) research community is heavily invested in the claim that programming with transactional memory is easier than alternatives (like locks), but evidence for or against the veracity of this claim is scant. In this paper, we describe a user-study in which 237 undergraduate students in an operating systems course implement the same programs using coarse and fine-grain locks, monitors, and transactions. We surveyed the students after the assignment, and examined their code to determine the types and frequency of programming errors for each synchronization technique. Inexperienced programmers found baroque syntax a barrier to entry for transactional programming. On average, subjective evaluation showed that students found transactions harder to use than coarse-grain locks, but slightly easier to use than fine-grained locks. Detailed examination of synchronization errors in the students' code tells a rather different story. Overwhelmingly, the number and types of programming errors the students made was much lower for transactions than for locks. On a similar programming problem, over 70\% of students made errors with fine-grained locking, while less than 10\% made errors with transactions.}
}

@comment{SSS}

@book{Sadowski2019,
  editor = {Caitlin Sadowski and Thomas Zimmermann},
  title = {Rethinking Productivity in Software Engineering},
  publisher = {Apress},
  year = {2019},
  isbn = {978-1484242209},
  url = {https://link.springer.com/book/10.1007\%2F978-1-4842-4221-6},
  abstract = {This open access book collects the wisdom of the 2017 Dagstuhl seminar on productivity in software engineering, a meeting of community leaders, who came together with the goal of rethinking traditional definitions and measures of productivity. The results of their work, Rethinking Productivity in Software Engineering, includes chapters covering definitions and core concepts related to productivity, guidelines for measuring productivity in specific contexts, best practices and pitfalls, and theories and open questions on productivity. You'll benefit from the many short chapters, each offering a focused discussion on one aspect of productivity in software engineering.}
}

@article{Scanniello2017,
  doi = {10.1145/3104029},
  url = {https://doi.org/10.1145/3104029},
  year = {2017},
  month = oct,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {26},
  number = {2},
  pages = {1--43},
  author = {Giuseppe Scanniello and Michele Risi and Porfirio Tramontana and Simone Romano},
  title = {Fixing Faults in C and Java Source Code},
  journal = {{ACM} Transactions on Software Engineering and Methodology},
  abstract = {We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications. We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word. We involved in this qualitative study six professional developers with 1--3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers.}
}

@inproceedings{Sedano2017,
  doi = {10.1109/icse.2017.20},
  url = {https://doi.org/10.1109/icse.2017.20},
  year = {2017},
  month = may,
  publisher = {{IEEE}},
  author = {Todd Sedano and Paul Ralph and Cecile Peraire},
  title = {Software Development Waste},
  booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering ({ICSE})},
  abstract = {Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.}
}

@article{Sharp2016,
  doi = {10.1109/tse.2016.2519887},
  url = {https://doi.org/10.1109/tse.2016.2519887},
  year = {2016},
  month = aug,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {42},
  number = {8},
  pages = {786--804},
  author = {Helen Sharp and Yvonne Dittrich and Cleidson R. B. de Souza},
  title = {The Role of Ethnographic Studies in Empirical Software Engineering},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.}
}

@inproceedings{Staples2013,
  doi = {10.1109/icse.2013.6606692},
  url = {https://doi.org/10.1109/icse.2013.6606692},
  year = {2013},
  month = may,
  publisher = {{IEEE}},
  author = {Mark Staples and Rafal Kolanski and Gerwin Klein and Corey Lewis and June Andronick and Toby Murray and Ross Jeffery and Len Bass},
  title = {Formal specifications better than function points for code sizing},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  abstract = {Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing.}
}

@inproceedings{Stefik2011,
  doi = {10.1145/2089155.2089159},
  url = {https://doi.org/10.1145/2089155.2089159},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Andreas Stefik and Susanna Siebert and Melissa Stefik and Kim Slattery},
  title = {An empirical comparison of the accuracy rates of novices using the Quorum, Perl, and Randomo programming languages},
  booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} workshop on Evaluation and usability of programming languages and tools - {PLATEAU} {\textquotesingle}11},
  abstract = {We present here an empirical study comparing the accuracy rates of novices writing software in three programming languages: Quorum, Perl, and Randomo. The first language, Quorum, we call an evidence-based programming language, where the syntax, semantics, and API designs change in correspondence to the latest academic research and literature on programming language usability. Second, while Perl is well known, we call Randomo a Placebo-language, where some of the syntax was chosen with a random number generator and the ASCII table. We compared novices that were programming for the first time using each of these languages, testing how accurately they could write simple programs using common program constructs (e.g., loops, conditionals, functions, variables, parameters). Results showed that while Quorum users were afforded significantly greater accuracy compared to those using Perl and Randomo, Perl users were unable to write programs more accurately than those using a language designed by chance.}
}

@article{Stefik2013,
  doi = {10.1145/2534973},
  url = {https://doi.org/10.1145/2534973},
  year = {2013},
  month = nov,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {13},
  number = {4},
  pages = {1--40},
  author = {Andreas Stefik and Susanna Siebert},
  title = {An Empirical Investigation into Programming Language Syntax},
  journal = {{ACM} Transactions on Computing Education},
  abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.}
}

@inproceedings{Stolee2011,
  doi = {10.1145/1985793.1985805},
  url = {https://doi.org/10.1145/1985793.1985805},
  year = {2011},
  month = may,
  publisher = {{ACM}},
  author = {Kathryn T. Stolee and Sebastian Elbaum},
  title = {Refactoring pipe-like mashups for end-user programmers},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  abstract = {Mashups are becoming increasingly popular as end users are able to easily access, manipulate, and compose data from many web sources. We have observed, however, that mashups tend to suffer from deficiencies that propagate as mashups are reused. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on end-user programmers and observe that users generally prefer mashups without smells. We then introduce refactorings targeting those smells, reducing the complexity of the mashup programs, increasing their abstraction, updating broken data sources and dated components, and standardizing their structures to fit the community development patterns. Our assessment of a large sample of mashups shows that smells are present in 81\% of them and that the proposed refactorings can reduce the number of smelly mashups to 16\%, illustrating the potential of refactoring to support the thousands of end users programming mashups.}
}

@inproceedings{Stylos2007,
  doi = {10.1109/icse.2007.92},
  url = {https://doi.org/10.1109/icse.2007.92},
  year = {2007},
  month = may,
  publisher = {{IEEE}},
  author = {Jeffrey Stylos and Steven Clarke},
  title = {Usability Implications of Requiring Parameters in Objects{\textquotesingle} Constructors},
  booktitle = {29th International Conference on Software Engineering ({ICSE}{\textquotesingle}07)},
  abstract = {The usability of APIs is increasingly important to programmer productivity. Based on experience with usability studies of specific APIs, techniques were explored for studying the usability of design choices common to many APIs. A comparative study was performed to assess how professional programmers use APIs with required parameters in objects' constructors as opposed to parameterless \"default\" constructors. It was hypothesized that required parameters would create more usable and self- documenting APIs by guiding programmers toward the correct use of objects and preventing errors. However, in the study, it was found that, contrary to expectations, programmers strongly preferred and were more effective with APIs that did not require constructor parameters. Participants' behavior was analyzed using the cognitive dimensions framework, and revealing that required constructor parameters interfere with common learning strategies, causing undesirable premature commitment.}
}

@comment{TTT}

@article{Taipalus2018,
  doi = {10.1145/3231712},
  url = {https://doi.org/10.1145/3231712},
  year = {2018},
  month = sep,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {18},
  number = {3},
  pages = {1--29},
  author = {Toni Taipalus and Mikko Siponen and Tero Vartiainen},
  title = {Errors and Complications in {SQL} Query Formulation},
  journal = {{ACM} Transactions on Computing Education},
  abstract = {SQL is taught in almost all university level database courses, yet SQL has received relatively little attention in educational research. In this study, we present a database management system independent categorization of SQL query errors that students make in an introductory database course. We base the categorization on previous literature, present a class of logical errors that has not been studied in detail, and review and complement these findings by analyzing over 33,000 SQL queries submitted by students. Our analysis verifies error findings presented in previous literature and reveals new types of errors, namely logical errors recurring in similar manners among different students. We present a listing of fundamental SQL query concepts we have identified and based our exercises on, a categorization of different errors and complications, and an operational model for designing SQL exercises.}
}

@inproceedings{Tew2011,
  doi = {10.1145/1953163.1953200},
  url = {https://doi.org/10.1145/1953163.1953200},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Allison Elliott Tew and Mark Guzdial},
  title = {The {FCS}1: a language independent assessment of CS1 knowledge},
  booktitle = {Proceedings of the 42nd {ACM} technical symposium on Computer science education - {SIGCSE} {\textquotesingle}11},
  abstract = {A primary goal of many CS education projects is to determine the extent to which a given intervention has had an impact on student learning. However, computing lacks valid assessments for pedagogical or research purposes. Without such valid assessments, it is difficult to accurately measure student learning or establish a relationship between the instructional setting and learning outcomes. We developed the Foundational CS1 (FCS1) Assessment instrument, the first assessment instrument for introductory computer science concepts that is applicable across a variety of current pedagogies and programming languages. We applied methods from educational and psychological test development, adapting them as necessary to fit the disciplinary context. We conducted a large scale empirical study to demonstrate that pseudo-code was an appropriate mechanism for achieving programming language independence. Finally, we established the validity of the assessment using a multi-faceted argument, combining interview data, statistical analysis of results on the assessment, and CS1 exam scores.}
}

@inproceedings{Thongtanunam2016,
  doi = {10.1145/2884781.2884852},
  url = {https://doi.org/10.1145/2884781.2884852},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Patanamon Thongtanunam and Shane McIntosh and Ahmed E. Hassan and Hajimu Iida},
  title = {Revisiting code ownership and its relationship with software quality in the scope of modern code review},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering},
  abstract = {Code ownership establishes a chain of responsibility for modules in large software systems. Although prior work uncovers a link between code ownership heuristics and software quality, these heuristics rely solely on the authorship of code changes. In addition to authoring code changes, developers also make important contributions to a module by reviewing code changes. Indeed, recent work shows that reviewers are highly active in modern code review processes, often suggesting alternative solutions or providing updates to the code changes. In this paper, we complement traditional code ownership heuristics using code review activity. Through a case study of six releases of the large Qt and OpenStack systems, we find that: (1) 67\%-86\% of developers did not author any code changes for a module, but still actively contributed by reviewing 21\%-39\% of the code changes, (2) code ownership heuristics that are aware of reviewing activity share a relationship with software quality, and (3) the proportion of reviewers without expertise shares a strong, increasing relationship with the likelihood of having post-release defects. Our results suggest that reviewing activity captures an important aspect of code ownership, and should be included in approximations of it in future studies.}
}

@comment{UUU}

@comment{VVV}

@inproceedings{Vanhanen2007,
  doi = {10.1109/hicss.2007.218},
  url = {https://doi.org/10.1109/hicss.2007.218},
  year = {2007},
  publisher = {{IEEE}},
  author = {Jari Vanhanen and Harri Korpi},
  title = {Experiences of Using Pair Programming in an Agile Project},
  booktitle = {2007 40th Annual Hawaii International Conference on System Sciences ({HICSS}{\textquotesingle}07)},
  abstract = {The interest in pair programming (PP) has increased recently, e.g. by the popularization of agile software development. However, many practicalities of PP are poorly understood. We present experiences of using PP extensively in an industrial project. The fact that the team had a limited number of high-end workstations forced it in a positive way to quick deployment and rigorous use of PP. The developers liked PP and learned it easily. Initially, the pairs were not rotated frequently but adopting daily, random rotation improved the situation. Frequent rotation seemed to improve knowledge transfer. The driver/navigator roles were switched seldom, but still the partners communicated actively. The navigator rarely spotted defects during coding, but the released code contained almost no defects. Test-driven development and design in pairs possibly decreased defects. The developers considered that PP improved quality and knowledge transfer, and was better suited for complex tasks than for easy tasks}
}

@comment{WWW}

@inproceedings{Wang2016,
  doi = {10.1145/2983990.2984030},
  url = {https://doi.org/10.1145/2983990.2984030},
  year = {2016},
  month = oct,
  publisher = {{ACM}},
  author = {Xinyu Wang and Sumit Gulwani and Rishabh Singh},
  title = {{FIDEX}: filtering spreadsheet data using examples},
  booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  abstract = {Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples. There are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0.22s), using only 2.2 positive string instances and 2.7 negative string instances on average.}
}

@inproceedings{Wang2020,
  doi = {10.1145/3379597.3387464},
  url = {https://doi.org/10.1145/3379597.3387464},
  year = {2020},
  month = jun,
  publisher = {{ACM}},
  author = {Peipei Wang and Chris Brown and Jamie A. Jennings and Kathryn T. Stolee},
  title = {An Empirical Study on Regular Expression Bugs},
  booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
  abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice. This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3\%). The remaining root causes are incorrect API usage (9.3\%) and other code issues that require regular expression changes in the fix (29.5\%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.}
}

@inproceedings{Washburn2016,
  doi = {10.1145/2889160.2889253},
  url = {https://doi.org/10.1145/2889160.2889253},
  year = {2016},
  month = may,
  publisher = {{ACM}},
  author = {Michael Washburn and Pavithra Sathiyanarayanan and Meiyappan Nagappan and Thomas Zimmermann and Christian Bird},
  title = {What went right and what went wrong: an analysis of 155 postmortems from game development},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
  abstract = {In game development, software teams often conduct postmortems to reflect on what went well and what went wrong in a project. The postmortems are shared publicly on gaming sites or at developer conferences. In this paper, we present an analysis of 155 postmortems published on the gaming site Gamasutra.com. We identify characteristics of game development, link the characteristics to positive and negative experiences in the postmortems and distill a set of best practices and pitfalls for game development.}
}

@article{Wicherts2011,
  doi = {10.1371/journal.pone.0026828},
  url = {https://doi.org/10.1371/journal.pone.0026828},
  year = {2011},
  month = nov,
  publisher = {Public Library of Science ({PLoS})},
  volume = {6},
  number = {11},
  pages = {e26828},
  author = {Jelte M. Wicherts and Marjan Bakker and Dylan Molenaar},
  editor = {Rochelle E. Tractenberg},
  title = {Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results},
  journal = {{PLoS} {ONE}},
  abstract = {Background The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.}
}

@article{Wilkerson2012,
  doi = {10.1109/tse.2011.46},
  url = {https://doi.org/10.1109/tse.2011.46},
  year = {2012},
  month = may,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {38},
  number = {3},
  pages = {547--560},
  author = {Jerod W. Wilkerson and Jay F. Nunamaker and Rick Mercer},
  title = {Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development},
  journal = {{IEEE} Transactions on Software Engineering},
  abstract = {This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.}
}

@comment{XXX}

@inproceedings{Xu2015,
  doi = {10.1145/2786805.2786852},
  url = {https://doi.org/10.1145/2786805.2786852},
  year = {2015},
  month = aug,
  publisher = {{ACM}},
  author = {Tianyin Xu and Long Jin and Xuepeng Fan and Yuanyuan Zhou and Shankar Pasupathy and Rukma Talwadker},
  title = {Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  abstract = {Configuration problems are not only prevalent, but also severely impair the reliability of today's system software. One fundamental reason is the ever-increasing complexity of configuration, reflected by the large number of configuration parameters (\"knobs\"). With hundreds of knobs, configuring system software to ensure high reliability and performance becomes a daunting, error-prone task. This paper makes a first step in understanding a fundamental question of configuration design: \"do users really need so many knobs?\" To provide the quantitatively answer, we study the configuration settings of real-world users, including thousands of customers of a commercial storage system (Storage-A), and hundreds of users of two widely-used open-source system software projects. Our study reveals a series of interesting findings to motivate software architects and developers to be more cautious and disciplined in configuration design. Motivated by these findings, we provide a few concrete, practical guidelines which can significantly reduce the configuration space. Take Storage-A as an example, the guidelines can remove 51.9\% of its parameters and simplify 19.7\% of the remaining ones with little impact on existing users. Also, we study the existing configuration navigation methods in the context of \"too many knobs\" to understand their effectiveness in dealing with the over-designed configuration, and to provide practices for building navigation support in system software.}
}

@comment{YYY}

@inproceedings{Yin2011,
  doi = {10.1145/2025113.2025121},
  url = {https://doi.org/10.1145/2025113.2025121},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Zuoning Yin and Ding Yuan and Yuanyuan Zhou and Shankar Pasupathy and Lakshmi Bairavasundaram},
  title = {How do fixes become bugs?},
  booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering - {SIGSOFT}/{FSE} {\textquotesingle}11},
  abstract = {Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation. This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8\%--24.4\% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39\% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27\% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process.}
}

@inproceedings{Yuan2014,
  doi = {10.13140/2.1.2044.2889},
  url = {https://doi.org/10.13140/2.1.2044.2889},
  author = {Ding Yuan and Yu Luo and Xin Zhuang and Guilherme {Renna Rodrigues} and Xu Zhao and Pranay U. Jain and Michael Stumm},
  title = {Simple Testing Can Prevent Most Critical Failures{\textemdash}An Analysis of Production Failures in Distributed Data-intensive Systems},
  booktitle = {11th USENIX Symposium on Operating System Design and Implementation (OSDI{\textquotesingle}14)},
  year = {2014},
  abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failures. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures---often with unit tests. We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code---the last line of defense---even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30\% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.}
}

@comment{ZZZ}

@inproceedings{Zhu2021,
  doi = {10.1109/msr52588.2021.00065},
  url = {https://doi.org/10.1109/msr52588.2021.00065},
  year = {2021},
  month = may,
  publisher = {{IEEE}},
  author = {Wenhan Zhu and Michael W. Godfrey},
  title = {Mea culpa: How developers fix their own simple bugs differently from other developers},
  booktitle = {2021 {IEEE}/{ACM} 18th International Conference on Mining Software Repositories ({MSR})},
  abstract = {In this work, we study how the authorship of code affects bug-fixing commits using the SStuBs dataset, a collection of single-statement bug fix changes in popular Java Maven projects. More specifically, we study the differences in characteristics between simple bug fixes by the original author---that is, the developer who submitted the bug-inducing commit---and by different developers (i.e., non-authors). Our study shows that nearly half (i.e., 44.3\%) of simple bugs are fixed by a different developer. We found that bug fixes by the original author and by different developers differed qualitatively and quantitatively. We observed that bug-fixing time by authors is much shorter than that of other developers. We also found that bug-fixing commits by authors tended to be larger in size and scope, and address multiple issues, whereas bug-fixing commits by other developers tended to be smaller and more focused on the bug itself. Future research can further study the different patterns in bug-fixing and create more tailored tools based on the developer's needs.}
}
